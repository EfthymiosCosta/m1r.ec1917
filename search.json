[
  {
    "objectID": "hclust.html",
    "href": "hclust.html",
    "title": "3. Hierarchical Clustering",
    "section": "",
    "text": "Definition (Hierarchical Clustering): A clustering algorithm that builds a nested hierarchy of non-overlapping, exhaustive clusters, where each data point belongs to exactly one cluster is called a hierarchical clustering algorithm."
  },
  {
    "objectID": "hclust.html#types-of-hierarchical-clustering",
    "href": "hclust.html#types-of-hierarchical-clustering",
    "title": "3. Hierarchical Clustering",
    "section": "Types of hierarchical clustering",
    "text": "Types of hierarchical clustering\nThere exist two ways to do hierarchical clustering:\n\nAgglomerative hierarchical clustering starts with \\(K = n\\) clusters (each observation forms its own cluster) and recursively merges clusters until just \\(K=1\\) cluster with all observations is obtained.\nDivisive hierarchical clustering starts with \\(K=1\\) cluster (all observations form a cluster together) and recursively splits clusters until \\(K=n\\) clusters are obtained, with each observation forming its own cluster.\n\nThe theory and implementations described below refer to agglomerative hierarchical clustering."
  },
  {
    "objectID": "hclust.html#agglomerative-clustering-implementation",
    "href": "hclust.html#agglomerative-clustering-implementation",
    "title": "3. Hierarchical Clustering",
    "section": "Agglomerative Clustering Implementation",
    "text": "Agglomerative Clustering Implementation\nWe present the steps used to perform hierarchical agglomerative clustering below:\n\nInitialise: Initialise \\(n\\) clusters \\(C_1 = \\{ 1\\}, \\ldots, C_n = \\{ n\\}\\) and \\(t \\leftarrow 1\\).\nCompute dissimilarities: Construct dissimilarity/distance matrix \\(D^{(0)}\\) with \\(D^{(0)}_{i,j} = d(\\mathbf{x}_i, \\mathbf{x}_j)\\) for a dissimilarity/distance function \\(d\\).\nRecursive steps: Repeat the following steps until \\(t = n-1\\):\n\n\n\nMerge: Find the two clusters that are least dissimilar and merge them, i.e.¬†merge clusters \\(C_{i^*}, C_{j^*}\\) where \\(i^*, j^*\\) are such that \\(D^{(t-1)}_{i^*,j^*} = \\min\\limits_{i \\neq j} D^{(t-1)}_{i,j}\\).\nUpdate dissimilarities: Update matrix of dissimilarities by computing cluster dissimilarities with the merged cluster, i.e.¬†\\([D^{(t)}]_{i,j} = d(C_i, C_j) \\ \\forall 1 \\leq i &lt; j \\leq n-t\\).\nIncrement: Set \\(t \\leftarrow t + 1\\).\n\n\n\nOutput: Cluster hierarchy.\n\n\nüí° Fun fact\nThe implementation described above is for any general dissimilarity/distance function \\(d\\). In fact, the R implementation of hierarchical agglomerative clustering requires a dissimilarity/distance matrix as input, not the full data set as e.g.¬†kmeans.\n\n\n‚ùì Dissimilarities between clusters?\nLook at step 3ii. of the above implementation; what does it mean to compute dissimilarities between two clusters \\(C_i\\) and \\(C_j\\)? How can we do that?"
  },
  {
    "objectID": "hclust.html#linkage-criteria",
    "href": "hclust.html#linkage-criteria",
    "title": "3. Hierarchical Clustering",
    "section": "Linkage Criteria",
    "text": "Linkage Criteria\nOne of the recursive steps of hierarchical agglomerative clustering involves constructing the matrix of dissimilarities so that an element \\((i, j)\\) represents the dissimilarity between clusters \\(C_i\\) and \\(C_j\\). However, clusters are (non-empty) sets of observations, so how can we apply a dissimilarity function on them? This can be done by specifying the dissimilarity of any two clusters as a function of the pairwise distances of observations included in these. This is precisely what a linkage criterion does! The table below displays some of the most commonly-used linkage criteria that can be used:\n\nLinkage criteria\n\n\n\n\n\n\nLinkage\nFormula\n\n\n\n\nComplete\n\\(\\max\\limits_{\\mathbf{x}_{i'} \\in C_i, \\mathbf{x}_{j'} \\in C_j}d(\\mathbf{x}_{i'}, \\mathbf{x}_{j'})\\)\n\n\nSingle\n\\(\\min\\limits_{\\mathbf{x}_{i'} \\in C_i, \\mathbf{x}_{j'} \\in C_j}d(\\mathbf{x}_{i'}, \\mathbf{x}_{j'})\\)\n\n\nAverage\n\\(\\frac{1}{\\lvert C_i \\rvert}\\frac{1}{\\lvert C_j \\rvert}\\sum\\limits_{\\mathbf{x}_{i'} \\in C_i}\\sum\\limits_{\\mathbf{x}_{j'} \\in C_j} d(\\mathbf{x}_{i'}, \\mathbf{x}_{j'})\\)\n\n\nWard\n\\(\\frac{\\lvert C_i \\rvert \\lvert C_j \\rvert}{\\lvert C_i \\cup C_j \\rvert} \\| \\mathbf{m}_i - \\mathbf{m}_j \\|^2\\), \\(\\mathbf{m}_k\\) is the centroid of cluster \\(C_k\\)\n\n\nCentroid\n\\(\\| \\mathbf{m}_i - \\mathbf{m}_j \\|^2\\), \\(\\mathbf{m}_k\\) is the centroid of cluster \\(C_k\\)\n\n\n\n\n\n\n\n\n\nNoteü§î Quiz: Let \\(C_1 = \\{1, 2\\}\\) and \\(C_2 = \\{3, 4\\}\\) , where:\n\n\n\n\\[\n\\mathbf{X} = \\begin{pmatrix}0 & 0 \\\\ 1 & -2 \\\\ 5 & -1 \\\\ 2 & 2\\end{pmatrix}.\n\\] What is the dissimilarity between \\(C_1\\) and \\(C_2\\) using the Euclidean distance and the single linkage?\nA. \\(\\sqrt{5}\\)\nB. \\(\\sqrt{8}\\)\nC. \\(\\sqrt{17}\\)\nD. \\(\\sqrt{26}\\)\n\n\n\nShow Answer\n\n\nüß† The answer is currently updating its linkage criteria‚Ä¶\nSingle linkage? Complete? Average? The answer is feeling indecisive. Check back after you‚Äôve computed all four distances!"
  },
  {
    "objectID": "hclust.html#hierarchical-agglomerative-clustering-in-r",
    "href": "hclust.html#hierarchical-agglomerative-clustering-in-r",
    "title": "3. Hierarchical Clustering",
    "section": "Hierarchical Agglomerative Clustering in R",
    "text": "Hierarchical Agglomerative Clustering in R\nHierarchical agglomerative clustering is available in R using the hclust function. This takes a dissimilarity matrix and a linkage criterion as input arguments. Let us look at an example on a synthetic data set about climate in 10 European cities. The data includes the average temperature, annual precipitation and sunshine hours recorded over a year. We use agglomerative clustering with the Euclidean distance and complete linkage to obtain a cluster hierarchy. Notice that we can plot the hierarchy using plot.\n\n# Create data set of European cities with their climate characteristics\ncities &lt;- data.frame(\n  avg_temp = c(9, 10, 15, 16, 18, 5, 6, 12, 14, 17),\n  annual_rain = c(600, 750, 500, 450, 400, 800, 850, 650, 550, 480),\n  sunshine_hours = c(1600, 1500, 2500, 2800, 2900, 1400, 1300, 2000, 2400, 2700),\n  row.names = c(\"London\", \"Paris\", \"Madrid\", \"Rome\", \"Athens\", \n                \"Stockholm\", \"Oslo\", \"Berlin\", \"Vienna\", \"Barcelona\")\n)\n\n# Perform hierarchical clustering\ndist_matrix &lt;- dist(scale(cities))\nhc &lt;- hclust(d = dist_matrix,\n             method = \"complete\")\n\n# Plot the hierarchy\nplot(hc, main = \"Hierarchical Clustering of European Cities by Climate\",\n     xlab = \"City\", ylab = \"Distance\", sub = \"\")\n\n\n\n\n\n\n\n\n\nüí° Fun fact\nThe plot of hierarchies is called a dendrogram and comes from the Greek word ‚ÄúŒ¥Œ≠ŒΩŒ¥œÅŒø‚Äù (dendro) which means ‚Äútree‚Äù. Do you see why a tree diagram is used here?\n\n\n‚ùì Scaled data?\nTake a look at the line where the dissimilarity matrix is constructed: dist_matrix &lt;- dist(scale(cities)). What does the scale function do? (Hint: Type ?scale in your console to get the function documentation). Do you understand why it is used?\n\nThe dendrogram above shows how the cluster hierarchy is formed. We can also inspect the output of our hierarchical clustering output hc.\n\nstr(hc)\n\nList of 7\n $ merge      : int [1:9, 1:2] -4 -3 -6 -5 -1 -2 2 3 7 -10 ...\n $ height     : num [1:9] 0.333 0.421 0.421 0.646 0.974 ...\n $ order      : int [1:10] 3 9 5 4 10 6 7 2 1 8\n $ labels     : chr [1:10] \"London\" \"Paris\" \"Madrid\" \"Rome\" ...\n $ method     : chr \"complete\"\n $ call       : language hclust(d = dist_matrix, method = \"complete\")\n $ dist.method: chr \"euclidean\"\n - attr(*, \"class\")= chr \"hclust\"\n\n\nTwo of the objects returned can help us understand how the clustering algorithm has proceeded:\n\nmerge: A matrix of dimensions \\((n-1) \\times 2\\) that describes the merges done. Positive values indicate clusters obtained by previous merges.\nheight: A vector of the \\(n-1\\) values describing the dissimilarity between the merged clusters at each merging step.\n\nFor instance, if we look at our data set, merge looks like this:\n\nprint(hc$merge)\n\n      [,1] [,2]\n [1,]   -4  -10\n [2,]   -3   -9\n [3,]   -6   -7\n [4,]   -5    1\n [5,]   -1   -8\n [6,]   -2    5\n [7,]    2    4\n [8,]    3    6\n [9,]    7    8\n\n\nThis tells us that the hierarchy was obtained as follows:\nStep 1: \\(\\{4\\} \\cup \\{10\\}\\) ‚Üí Cluster 1\nStep 2: \\(\\{3\\} \\cup \\{9\\}\\) ‚Üí Cluster 2\nStep 3: \\(\\{6\\} \\cup \\{7\\}\\) ‚Üí Cluster 3\nStep 4: \\(\\{5\\} \\cup\\) Cluster 1 ‚Üí Cluster 4 (\\(\\{4, 10, 5\\}\\))\nStep 5: \\(\\{1\\} \\cup \\{8\\}\\) ‚Üí Cluster 5\nStep 6: \\(\\{2\\} \\cup\\) Cluster 5 ‚Üí Cluster 6 (\\(\\{1, 8, 2\\}\\))\nStep 7: Cluster 2 + Cluster 4 ‚Üí Cluster 7 (\\(\\{3,9,4,10,5\\}\\))\nStep 8: Cluster 3 + Cluster 6 ‚Üí Cluster 8 (\\(\\{6,7,1,8,2\\}\\))\nStep 9: Cluster 7 + Cluster 8 ‚Üí Final cluster (\\(\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\\}\\))\nThe associated dissimilarities that led to these merges are:\n\nprint(hc$height)\n\n[1] 0.3333612 0.4213636 0.4213636 0.6456208 0.9737931 1.1188796 1.5318198\n[8] 2.1567316 4.6816562\n\n\n\n‚ùì Non-decreasing height values\nThe values in height are always non-decreasing. Can you explain why?\n\nWe now have our cluster hierarchy but how do we get access to the clusters? The idea is that we can ‚Äúcut the tree‚Äù structure we got in the dendrogram and look at the clusters obtained. We can either cut at a specific height, or enforce a specific number of clusters. For instance, in the chunk below, we cut the tree into \\(K=3\\) clusters and then we also cut it at a height of 3. Both actions can be performed using the cutree function. We can also visualise the obtained partitions on the dendrogram with rect.hclust.\n\n# Cut to get 3 clusters\nhc_clust1 &lt;- cutree(tree = hc,\n                    k = 3)\n# Cut at a height of 3\nhc_clust2 &lt;- cutree(tree = hc,\n                    h = 3)\n\n# Plot the hierarchy and obtained partitions\nplot(hc, main = \"Hierarchical Clustering of European Cities by Climate\",\n     xlab = \"City\", ylab = \"Distance\", sub = \"\")\nrect.hclust(hc, k = 3, border = c(\"red\", \"blue\", \"darkgreen\"))\nrect.hclust(hc, h = 3, border = c(\"magenta\", \"orange\"))\n\n\n\n\n\n\n\n\nAs we can see, a height of 3 has returned a partition into \\(K=2\\) clusters. In general, cutting a tree at a given height is not straightforward and requires inspecting the dendrogram in advance. This is why we normally prefer to cut in a way that allows specifying the number of clusters. Notice that cutting the tree at different heights may lead to the exact same partition in many cases.\n\n‚ùì Agglomerative vs.¬†Divisive hierarchical clustering\nAgglomerative clustering is sometimes referred to as a ‚Äúbottom-up‚Äù approach, whereas the term ‚Äútop-down‚Äù is used to describe divisive clustering. Can you explain why? Relate your answer to the dendrogram and what it illustrates.\n\n\nüöÄ Time to practice!\nThe mtcars data set includes eleven features related to the design and the performance of 32 cars. Load the mtcars data in R (just use data(iris) to load it to your working environment). Keep only the purely numeric variables (i.e.¬†remove the two binary variables vs and am) and perform hierarchical clustering with different combinations of dissimilarity functions and linkage criteria. Plot dendrograms for some of the obtained cluster hierarchies and try to identify whether your partitions make sense for different numbers of clusters.\n(You may find it helpful to look at the car model names by typing row.names(mtcars). Documentation for the data set is available by typing ?mtcars)."
  },
  {
    "objectID": "hclust.html#key-takeaways",
    "href": "hclust.html#key-takeaways",
    "title": "3. Hierarchical Clustering",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nHierarchical clustering algorithms produce a hierarchy of partitions from \\(K=1\\) up to \\(K=n\\) exhaustive, non-overlapping clusters.\nThere are 2 main approaches to hierarchical clustering; agglomerative and divisive.\nHierarchical clustering can work with any dissimilarity function.\nLinkage criteria specify the dissimilarity of any two clusters as a function of the pairwise distances of observations included in these.\nCluster hierarchies can be visualised using dendrograms.\nHierarchical agglomerative clustering is implemented in R using hclust."
  },
  {
    "objectID": "kmeans_pam.html",
    "href": "kmeans_pam.html",
    "title": "2. Partitional Clustering",
    "section": "",
    "text": "Definition (Partitional Clustering): A clustering algorithm that divides a data set into a fixed number \\(k\\) of non-overlapping, exhaustive clusters, where each data point belongs to exactly one cluster is called a partitional clustering algorithm."
  },
  {
    "objectID": "kmeans_pam.html#k-means",
    "href": "kmeans_pam.html#k-means",
    "title": "2. Partitional Clustering",
    "section": "K-Means",
    "text": "K-Means\nThis is probably the most commonly used and studied clustering algorithm. It is simple, easy to understand and implement, and produces easy-to-interpret clusters.\nDefinition (K-Means): K-Means clustering is an iterative, partitional clustering algorithm that partitions \\(n\\) data points into \\(K\\) pre-defined, non-overlapping clusters, where each point belongs to the cluster with the nearest mean, minimising the within-cluster variance.\nMore specifically, given a data set of \\(n\\) continuous \\(p\\)-dimensional observations \\(\\mathbf{X}\\), the K-Means algorithm seeks to find a partition of the data points into \\(K\\) groups. This partition, denoted by \\(\\mathcal{C} = \\{C_1, \\ldots, C_K\\}\\), is chosen among the set of all partitions into \\(K\\) non-empty groups \\(\\mathcal{P}_K\\) to minimise the within-cluster sum of squares, which acts as a measure of cluster variability. More formally, the K-Means objective is formulated as follows:\n\\[\n\\mathcal{C} = \\argmin\\limits_{\\mathcal{C}' \\in \\mathcal{P}_K } \\sum\\limits_{k=1}^K \\sum\\limits_{i : \\mathbf{x}_i \\in C'_k} \\| \\mathbf{x}_i - \\mathbf{m}_k \\|^2, \\quad \\mathbf{m}_k = \\frac{1}{\\lvert C'_k \\rvert} \\sum\\limits_{i: \\mathbf{x}_i \\in C'_k} \\mathbf{x}_i.\n\\]\n\nüí° Fun fact\nThe name ‚ÄúK-Means‚Äù is not random. The algorithm was named like that because it seeks to minimise the distance between points and their respective cluster‚Äôs mean, denoted by \\(\\mathbf{m}_k\\) for cluster \\(1 \\leq k \\leq K\\) above. The \\(\\mathbf{m}_k\\)‚Äôs are called the cluster centroids as they are the most centrally-located observations in their cluster. Can you prove this result?\n(Hint: Fix a cluster \\(k\\) and differentiate with respect to \\(\\mathbf{m}_k\\). Do you get \\(\\mathbf{m}_k = \\frac{1}{\\lvert C'_k \\rvert} \\sum\\limits_{i: \\mathbf{x}_i \\in C'_k} \\mathbf{x}_i\\)?)\n\n\n‚ùì Do I have to use the Euclidean distance?\nNo! You can use any distance or dissimilarity function of your choice! For instance, if you use the Manhattan distance, you will implement the K-Medians clustering algorithm. (Can you see why it is called ‚ÄúK-Medians‚Äù?)"
  },
  {
    "objectID": "kmeans_pam.html#k-means-implementation",
    "href": "kmeans_pam.html#k-means-implementation",
    "title": "2. Partitional Clustering",
    "section": "K-Means Implementation",
    "text": "K-Means Implementation\nK-Means is an iterative algorithm, that is, it uses an iterative relocation process that updates the cluster assignments until these remain unchanged. Here are the steps of K-Means:\n\nInitialise: Choose \\(K\\) initial centroids \\(\\mathbf{m}_1^{(0)}, \\ldots, \\mathbf{m}_K^{(0)}\\), set \\(t \\leftarrow 1\\) and converged \\(\\leftarrow\\) FALSE.\nIterative steps: While \\(t \\leq t^\\text{max}\\) and converged \\(\\neq\\) TRUE do:\n\n\n\nAssign: Assign each observation to the cluster with its nearest centroid: \\[\nC_k^{(t)} = \\{\\mathbf{x}: \\| \\mathbf{x} - \\mathbf{m}_k^{(t-1)} \\| \\leq \\| \\mathbf{x} - \\mathbf{m}_j^{(t-1)} \\| \\ \\forall k \\neq j \\}.\n\\]\nUpdate: Update the cluster centroids by computing the cluster means for the updated cluster assignments: \\[\n\\mathbf{m}_k^{(t)} = \\frac{1}{\\lvert C_k^{(t)}\\rvert }\\sum\\limits_{\\mathbf{x} \\in C_k^{(t)}} \\mathbf{x}\n\\]\nConvergence check: If \\(\\mathbf{m}_k^{(t)} = \\mathbf{m}_k^{(t-1)} \\ \\forall 1 \\leq k \\leq K\\), set converged \\(\\leftarrow\\) TRUE.\nIncrement: Set \\(t \\leftarrow t + 1\\).\n\n\n\nOutput: Partition \\(\\mathcal{C} = \\{C_1, \\ldots, C_K \\}\\), centroids \\(\\mathbf{m}_1, \\ldots, \\mathbf{m}_K\\).\n\nThe iterative process in Step 2 is repeated until either convergence is achieved (i.e.¬†cluster assignments remain unchanged) or until a maximum number of iterations \\(t^\\text{max}\\) is reached. The K-Means algorithm usually converges in a few steps, so it‚Äôs only a matter of choosing \\(t^\\text{max}\\) not to be too small (e.g. \\(t^\\text{max} = 2\\) is not enough)."
  },
  {
    "objectID": "kmeans_pam.html#k-means-in-r",
    "href": "kmeans_pam.html#k-means-in-r",
    "title": "2. Partitional Clustering",
    "section": "K-Means in R",
    "text": "K-Means in R\nK-Means is implemented using the kmeans function in R. We generate a toy example to illustrate how the function works. The code below generates a data set with three well-separated clusters and runs K-Means with 3 clusters (input argument centers = 3), allowing up to 100 iterations until convergence (input argument iter.max = 100).\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create a data set with 3 well-separated clusters with 50 observations each\nn &lt;- 50\ncluster1 &lt;- data.frame(x = rnorm(n, mean = 0, sd = 0.5),\n                       y = rnorm(n, mean = 0, sd = 0.5))\ncluster2 &lt;- data.frame(x = rnorm(n, mean = 4, sd = 0.5),\n                       y = rnorm(n, mean = 0, sd = 0.5))\ncluster3 &lt;- data.frame(x = rnorm(n, mean = 2, sd = 0.5),\n                       y = rnorm(n, mean = 3, sd = 0.5))\n\ndata &lt;- rbind(cluster1, cluster2, cluster3)\n\n# Set the true labels (will not be used but good to have)\ntrue_labels &lt;- rep(1:3, each = n)\n\ndata_kmeans &lt;- kmeans(x = data,\n                      centers = 3,\n                      iter.max = 100)\n\n# Inspect kmeans object structure\nstr(data_kmeans)\n\nList of 9\n $ cluster     : int [1:150] 3 3 3 3 3 3 3 3 3 3 ...\n $ centers     : num [1:3, 1:2] 1.9957 3.873 0.0172 3.1247 0.0194 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"1\" \"2\" \"3\"\n  .. ..$ : chr [1:2] \"x\" \"y\"\n $ totss       : num 753\n $ withinss    : num [1:3] 21.9 22.6 20.5\n $ tot.withinss: num 65.1\n $ betweenss   : num 688\n $ size        : int [1:3] 50 50 50\n $ iter        : int 2\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n\n\nüí° Fun fact\nThe input argument centers in the kmeans function can either take an integer that represents the number of clusters, or initial cluster centroids If an integer value is provided (as we did above), the initial centroids are chosen randomly.\n\nSome interesting things included in the output are:\n\ncluster: The clustering output. A vector of integers from 1 up to \\(K\\) (in this case \\(K=3\\)) indicating the cluster each observation is assigned to.\ncenters: The cluster centroids; a \\((K \\times p)\\)-dimensional matrix, where row \\(k\\) represents \\(\\mathbf{m}_k\\).\nsize: A \\(K\\)-dimensional vector including the size of each cluster, i.e.¬†number of observations assigned to each cluster.\niter: The number of iterations the algorithm needed to reach convergence.\n\nThe output of the kmeans function includes some sum of squares values, specifically the between-cluster sum of squares (betweenss) and the within-cluster sum of squares (withinss). These are metrics which are related to how ‚Äútight‚Äù and how ‚Äúseparated‚Äù the clusters are. Specifically:\n\\[\n\\begin{align*}\n\\text{BCSS} = \\sum\\limits_{k=1}^K \\lvert C_k \\rvert \\| \\mathbf{m}_k - \\bar{\\mathbf{x}} \\|^2,\\\\\n\\text{WCSS} = \\sum\\limits_{k=1}^K \\sum\\limits_{\\mathbf{x}_i \\in C_k} \\| \\mathbf{x}_i - \\mathbf{m}_k \\|^2,\n\\end{align*}\n\\] with \\(\\bar{\\mathbf{x}} \\|^2\\) representing the overall mean vector of the whole data set.\n\n‚ùì How are these related to the K-Means objective?\nCan you see how the BCSS and the WCSS relate to the objective of K-Means?"
  },
  {
    "objectID": "kmeans_pam.html#initialisation-matters",
    "href": "kmeans_pam.html#initialisation-matters",
    "title": "2. Partitional Clustering",
    "section": "Initialisation matters",
    "text": "Initialisation matters\nK-Means is sensitive to initialisation. The code below runs K-Means on the toy data set we generated above using two different initialisations. It then plots the cluster solutions in a scatter plot, where each point is coloured according to the cluster it has been assigned to.\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# Bad initialisation; all initial centers are close together\nbad_centers &lt;- rbind(c(1.8, 2.8), c(2.0, 3.0), c(2.2, 3.2))\nkm_bad &lt;- kmeans(data, centers = bad_centers, iter.max = 100)\n\n# Good initialisation; spread centers across the data\ngood_centers &lt;- rbind(c(0, 0), c(4, 0), c(2, 3))\nkm_good &lt;- kmeans(data, centers = good_centers, iter.max = 100)\n\n# Convert integer values to factors\ndata_copy &lt;- data\ndata_copy$bad_cluster &lt;- factor(km_bad$cluster)\ndata_copy$good_cluster &lt;- factor(km_good$cluster)\n\n# Convert centers to data frames with proper column names\nbad_centers_df &lt;- as.data.frame(km_bad$centers)\nbad_centers_df$cluster &lt;- factor(1:nrow(bad_centers_df))\nnames(bad_centers_df) &lt;- c(\"x\", \"y\", \"cluster\")\n\ngood_centers_df &lt;- as.data.frame(km_good$centers)\ngood_centers_df$cluster &lt;- factor(1:nrow(good_centers_df))\nnames(good_centers_df) &lt;- c(\"x\", \"y\", \"cluster\")\n\n# Create plots\np1 &lt;- ggplot(data_copy, aes(x = x, y = y, color = bad_cluster)) +\n  geom_point(size = 2) +\n  geom_point(data = bad_centers_df, \n             aes(x = x, y = y), \n             color = \"black\", size = 5, shape = 4, stroke = 2.5) +\n  geom_point(data = bad_centers_df, \n             aes(x = x, y = y, color = cluster), \n             size = 4, shape = 4, stroke = 2) + \n  labs(title = \"Bad initialisation solution\",\n       subtitle = paste0(\"Total WCSS: \", round(km_bad$tot.withinss, 2),\n                        \",\\nBCSS: \", round(km_bad$betweenss, 2))) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\np2 &lt;- ggplot(data_copy, aes(x = x, y = y, color = good_cluster)) +\n  geom_point(size = 2) +\n  geom_point(data = good_centers_df, \n             aes(x = x, y = y), \n             color = \"black\", size = 5, shape = 4, stroke = 2.5) +\n  geom_point(data = good_centers_df, \n             aes(x = x, y = y, color = cluster), \n             size = 4, shape = 4, stroke = 2) + \n  labs(title = \"Good initialisation solution\",\n       subtitle = paste0(\"Total WCSS: \", round(km_good$tot.withinss, 2),\n                        \",\\nBCSS: \", round(km_good$betweenss, 2))) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# Combine plots\np1 / p2\n\n\n\n\n\n\n\n\n\n‚ùì Can you explain what is happening?\nThe crosses above represent the cluster centroids obtained with the 2 different initialisations. Can you explain what has happened and why these are different? If you were to select one of the two solutions, which one would you choose? What does this tell us about the BCSS and the WCSS?\n\nThe example above shows that K-Means can be very sensitive to initialisation. There are two ways that this can be alleviated:\n\nManually give a good initialisation (harder but quicker)\nTry multiple random initialisations (easier but slower)\n\nThe first option is harder because it requires having an idea of what the cluster centroids are in advance of performing clustering. But this is a bit counterintuitive, as the ultimate goal of clustering is to detect group structures that we do not know beforehand. This is why multiple initialisations (i.e.¬†the second option) is almost always preferred. This can be significantly more expensive, as it involves re-running the algorithm and then comparing all solutions, but kmeans is a highly optimised function that runs very fast for a large number of resimulations. The number of times the function runs with different random initial centroids is controlled via nstart. Once solutions have been obtained for all random initialisations, the optimal solution that is returned is the one with the lowest WCSS and the highest BCSS (favouring well-separated compact clusters).\nWe run the example from above with 100 random initialisations in the chunk below.\n\n# Multiple initialisations; spread centers across the data\nkm_multi &lt;- kmeans(data,\n                   centers = 3,\n                   nstart = 100,\n                   iter.max = 100)\n\n# Convert integer values to factors\ndata_copy$multi_clust &lt;- factor(km_multi$cluster)\n\n# Convert centers to data frames with proper column names\nmulti_clust_df &lt;- as.data.frame(km_multi$centers)\nmulti_clust_df$cluster &lt;- factor(1:nrow(multi_clust_df))\nnames(multi_clust_df) &lt;- c(\"x\", \"y\", \"cluster\")\n\n# Create plot\np3 &lt;- ggplot(data_copy, aes(x = x, y = y, color = multi_clust)) +\n  geom_point(size = 2) +\n  geom_point(data = multi_clust_df, \n             aes(x = x, y = y), \n             color = \"black\", size = 5, shape = 4, stroke = 2.5) +\n  geom_point(data = multi_clust_df, \n             aes(x = x, y = y, color = cluster), \n             size = 4, shape = 4, stroke = 2) + \n  labs(title = \"Multiple initialisations solution\",\n       subtitle = paste0(\"Total WCSS: \", round(km_multi$tot.withinss, 2),\n                        \",\\nBCSS: \", round(km_multi$betweenss, 2))) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# Combine plots\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\n‚ùì Different output each time?\nIn the plot above, the good initialisation and the multiple initialisations solutions produce different clustering labels. If you rerun kmeans with multiple initialisations on the data, you will see that these labels keep switching. Is this a problem?"
  },
  {
    "objectID": "kmeans_pam.html#k-medoids",
    "href": "kmeans_pam.html#k-medoids",
    "title": "2. Partitional Clustering",
    "section": "K-Medoids",
    "text": "K-Medoids\nDefinition (K-Medoids): K-Medoids is an iterative, partitional clustering algorithm that partitions \\(n\\) data points into \\(K\\) pre-defined, non-overlapping clusters, where each cluster is represented by one of its actual data points (called a medoid), and each point is assigned to the cluster whose medoid is closest to it with respect to a given dissimilarity function.\nK-Medoids is flexible in the sense that it allows for any dissimilarity function to be used.\n\n‚ùì Isn‚Äôt that just K-Means when we use the Euclidean distance?\nActually no! The main difference here is that K-Medoids uses actual observations as the ‚Äúcluster-representative points‚Äù (medoids), whereas the centroid in K-Means can (and usually is) some arbitrary point in \\(\\mathbb{R}^p\\). Can you think of a case where that could make a difference?"
  },
  {
    "objectID": "kmeans_pam.html#k-medoids-implementation",
    "href": "kmeans_pam.html#k-medoids-implementation",
    "title": "2. Partitional Clustering",
    "section": "K-Medoids Implementation",
    "text": "K-Medoids Implementation\nWe present an implementation of K-Medoids called Partitioning Around Medoids (PAM). Other implementations also exist but this is probably the most well-known and most widely used one.\n\nInitialise: Choose \\(K\\) initial medoids \\(M^{(0)} = \\{\\mathbf{m}_1^{(0)}, \\ldots, \\mathbf{m}_K^{(0)}\\}\\), set \\(t \\leftarrow 1\\) and converged \\(\\leftarrow\\) FALSE.\nIterative steps: While \\(t \\leq t^\\text{max}\\) and converged \\(\\neq\\) TRUE do:\n\n\n\nAssign: Assign each point to nearest medoid: \\[\n   C^{(t)}_k = \\{\\mathbf{x} \\in \\mathbf{X} : d(\\mathbf{x}, \\mathbf{m}^{(t-1)}_k) \\leq d(\\mathbf{x}, \\mathbf{m}^{(t-1)}_j) \\ \\forall j \\neq k\\}\n   \\]\nUpdate/Swap: Compute total cost change \\(\\Delta_{ik}^{(t)}\\) of swapping each medoid \\(\\mathbf{m}_k^{(t)}\\) to observation \\(\\mathbf{x}_i\\) for all non-medoids \\(\\mathbf{x}_i \\in \\mathbf{X} \\backslash M^{(t-1)}\\). If \\(\\min\\limits_{k,i} \\Delta_{ik}^{(t)} &lt; 0\\), perform the best swap to update medoids \\(M^{(t)}\\).\nConvergence check: If \\(\\mathbf{m}_k^{(t)} = \\mathbf{m}_k^{(t-1)} \\ \\forall 1 \\leq k \\leq K\\), set converged \\(\\leftarrow\\) TRUE.\nIncrement: Set \\(t \\leftarrow t + 1\\).\n\n\n\nOutput: Final partition and medoids.\n\nNotice that the cost change \\(\\Delta_{ik}\\) is given by:\n\\[\n\\Delta_{ik}^{(t)} = \\sum_{s=1}^n \\left[ \\min_{\\ell=1,\\ldots,K} d\\left(\\mathbf{x}_s, \\widetilde{\\mathbf{m}}_\\ell^{(ik)}\\right) - \\min_{\\ell=1,\\ldots,K} d\\left(\\mathbf{x}_s, \\mathbf{m}_\\ell^{(t-1)}\\right) \\right]\n\\] where \\(d\\) is a dissimilarity function and \\(\\widetilde{\\mathbf{m}}_\\ell^{(ik)} = \\mathbf{x}_i\\) if \\(\\ell = k\\), else \\(\\widetilde{\\mathbf{m}}_\\ell^{(ik)} = \\mathbf{m}_\\ell^{(t-1)}\\). This computes the total change in dissimilarity if we swap medoid \\(\\mathbf{m}_k^{(t-1)}\\) with observation \\(\\mathbf{x}_i\\). The first term is the minimum distance to medoids after the swap, the second term is the minimum distance before the swap.\n\nüí° Fun fact\nUnlike K-means which computes centroids as means, PAM tests every possible swap between current medoids and non-medoids. This corresponds to \\(K \\times (n-K)\\) swaps per iteration! The swap that reduces the total dissimilarity the most is chosen in each updating step. This exhaustive search makes PAM more computationally expensive."
  },
  {
    "objectID": "kmeans_pam.html#k-medoids-in-r",
    "href": "kmeans_pam.html#k-medoids-in-r",
    "title": "2. Partitional Clustering",
    "section": "K-Medoids in R",
    "text": "K-Medoids in R\nK-Medoids, and more specifically PAM, is implemented using the pam function from the cluster package in R. We will use the same toy data set as before with 3 clusters (input argument k = 3) and the Euclidean distance as our dissimilarity (diss = \"euclidean\"), with 100 random medoid initialisations (nstart = 100).\n\nlibrary(cluster)\ndata_pam_euclid &lt;- pam(x = data,\n                       k = 3,\n                       metric = \"euclidean\",\n                       nstart = 100)\nstr(data_pam_euclid)\n\nList of 10\n $ medoids   : num [1:3, 1:2] -0.1151 3.7547 1.9085 -0.0143 -0.017 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"x\" \"y\"\n $ id.med    : int [1:3] 2 73 137\n $ clustering: int [1:150] 1 1 1 1 1 1 1 1 1 1 ...\n $ objective : Named num [1:2] 2.729 0.593\n  ..- attr(*, \"names\")= chr [1:2] \"build\" \"swap\"\n $ isolation : Factor w/ 3 levels \"no\",\"L\",\"L*\": 1 1 1\n  ..- attr(*, \"names\")= chr [1:3] \"1\" \"2\" \"3\"\n $ clusinfo  : num [1:3, 1:5] 50 50 50 1.24 1.65 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:5] \"size\" \"max_diss\" \"av_diss\" \"diameter\" ...\n $ silinfo   :List of 3\n  ..$ widths         : num [1:150, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"2\" \"5\" \"39\" \"10\" ...\n  .. .. ..$ : chr [1:3] \"cluster\" \"neighbor\" \"sil_width\"\n  ..$ clus.avg.widths: num [1:3] 0.767 0.758 0.752\n  ..$ avg.width      : num 0.759\n $ diss      : NULL\n $ call      : language pam(x = data, k = 3, metric = \"euclidean\", nstart = 100)\n $ data      : num [1:150, 1:2] -0.2802 -0.1151 0.7794 0.0353 0.0646 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"x\" \"y\"\n - attr(*, \"class\")= chr [1:2] \"pam\" \"partition\"\n\n\nWe see that there are several things included in the output of the pam function. The ones to mainly keep in mind are clustering, which corresponds to the cluster allocation, and medoids, which includes the cluster medoids (these are actual observations in our data set). We also provide an illustration of the result below:\n\n# Convert integer values to factors\ndata_copy$pam_euclid &lt;- factor(data_pam_euclid$clustering)\n\n# Convert centers to data frames with proper column names\npam_euclid_df &lt;- as.data.frame(data_pam_euclid$medoids)\npam_euclid_df$cluster &lt;- factor(1:nrow(pam_euclid_df))\nnames(pam_euclid_df) &lt;- c(\"x\", \"y\", \"cluster\")\n\n# Create plot\np4 &lt;- ggplot(data_copy, aes(x = x, y = y, color = pam_euclid)) +\n  geom_point(size = 2) +\n  geom_point(data = pam_euclid_df, \n             aes(x = x, y = y), \n             color = \"black\", size = 5, shape = 4, stroke = 2.5) +\n  geom_point(data = pam_euclid_df, \n             aes(x = x, y = y, color = cluster), \n             size = 4, shape = 4, stroke = 2) + \n  labs(title = \"PAM solution with Euclidean distance\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# Combine plots\np3 / p4\n\n\n\n\n\n\n\n\n\n‚ùì Is there no stopping criterion in PAM?\nYes, this is either convergence of the algorithm or exceeding \\(t^\\text{max}\\) as mentioned in the earlier steps. However, the pam function does not include the latter and runs until it reaches convergence.\n\nAlternative dissimilarities can be used for PAM. The metric input argument only accepts \"euclidean\" or \"manhattan\", in which case a dissimilarity matrix is constructed internally and it is then used for cluster allocation. However, let us suppose we wish to use the Minkowski distance with power \\(p = 3\\). In such a case, we can construct the dissimilarity/distance matrix with the dist function and pass it as our main input argument x to pam, setting diss = TRUE to indicate that the main input argument x is a dissimilarity/distance matrix. Notice that when you input a distance/matrix, the medoids output only gives the observation indices for the medoids and not the actual points.\n\nmink3_distmat &lt;- dist(x = data,\n                      method = \"minkowski\",\n                      p = 3)\ndata_pam_mink3 &lt;- pam(x = mink3_distmat,\n                      k = 3,\n                      diss = TRUE,\n                      nstart = 100)\n# Convert integer values to factors\ndata_copy$pam_mink3 &lt;- factor(data_pam_mink3$clustering)\n\n# Convert centers to data frames with proper column names\npam_mink3_df &lt;- as.data.frame(data[data_pam_mink3$medoids, ])\npam_mink3_df$cluster &lt;- factor(1:nrow(pam_mink3_df))\nnames(pam_mink3_df) &lt;- c(\"x\", \"y\", \"cluster\")\n\n# Create plot\np5 &lt;- ggplot(data_copy, aes(x = x, y = y, color = pam_mink3)) +\n  geom_point(size = 2) +\n  geom_point(data = pam_mink3_df, \n             aes(x = x, y = y), \n             color = \"black\", size = 5, shape = 4, stroke = 2.5) +\n  geom_point(data = pam_mink3_df, \n             aes(x = x, y = y, color = cluster), \n             size = 4, shape = 4, stroke = 2) + \n  labs(title = \"PAM solution with Minkowski (p = 3) distance\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# Combine plots\np3 / p4 / p5\n\n\n\n\n\n\n\n\n\nüöÄ Time to practice!\nThe iris data set includes three iris species with 50 samples each as well as some properties about each flower. Load the iris data in R (just use data(iris) to load it to your working environment). Remove the final column that corresponds to the Species of the iris flowers and use the remaining columns to run clustering. Use K-Means or PAM with dissimilarity functions of your choice and compare your results. Change some of the input arguments of kmeans and pam and check what happens.\n(You may find it helpful to look up the documentation of kmeans and pam by typing ?kmeans and ?pam in your console)."
  },
  {
    "objectID": "kmeans_pam.html#key-takeaways",
    "href": "kmeans_pam.html#key-takeaways",
    "title": "2. Partitional Clustering",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nPartitional clustering algorithms divide a data set into a fixed number of exhaustive, non-overlapping clusters.\nK-Means is a partitional clustering algorithm that seeks to minimise the within-cluster-variance.\nK-Medoids is a generalisation of K-Means to use any dissimilarity function but requires cluster representative points to be actual observations from the data set.\nBoth K-Means and K-Medians are sensitive to initialisation.\nK-Means and K-Medoids are implemented in R using kmeans and pam, respectively."
  },
  {
    "objectID": "projectideas.html",
    "href": "projectideas.html",
    "title": "Project Ideas",
    "section": "",
    "text": "You are more than welcome to work on any project of your choice, as long as it is relevant to clustering and the materials presented. A common structure of your project c ould be the following:\n\nChoose a data set of your choice that is interesting to you (and explain why).\nPerform cluster analysis, estimate the number of clusters.\nInterpret your results and identify interesting patterns.\n\nIf you would like to stick with more methodological (but perhaps a bit more complicated) projects, the list below includes some suggestions on topics you may wish to work on for your poster:\n\nComparison of partitional/hierarchical clustering with different dissimilarity functions/linkage criteria.\nSelection of the number of clusters in a data set using multiple intrinsic evaluation metrics.\nAssessment of the ‚Äúclusterability‚Äù of multiple data sets using multiple clustering algorithms.\nDevelopment of a dissimilarity function that allows for more interpretable clusters.\nComparison of algorithmic complexity for clustering algorithms.\n\n(The list above is by no means exhaustive)."
  },
  {
    "objectID": "projectideas.html#suggestions",
    "href": "projectideas.html#suggestions",
    "title": "Project Ideas",
    "section": "",
    "text": "You are more than welcome to work on any project of your choice, as long as it is relevant to clustering and the materials presented. A common structure of your project c ould be the following:\n\nChoose a data set of your choice that is interesting to you (and explain why).\nPerform cluster analysis, estimate the number of clusters.\nInterpret your results and identify interesting patterns.\n\nIf you would like to stick with more methodological (but perhaps a bit more complicated) projects, the list below includes some suggestions on topics you may wish to work on for your poster:\n\nComparison of partitional/hierarchical clustering with different dissimilarity functions/linkage criteria.\nSelection of the number of clusters in a data set using multiple intrinsic evaluation metrics.\nAssessment of the ‚Äúclusterability‚Äù of multiple data sets using multiple clustering algorithms.\nDevelopment of a dissimilarity function that allows for more interpretable clusters.\nComparison of algorithmic complexity for clustering algorithms.\n\n(The list above is by no means exhaustive)."
  },
  {
    "objectID": "projectideas.html#finding-data",
    "href": "projectideas.html#finding-data",
    "title": "Project Ideas",
    "section": "Finding data",
    "text": "Finding data\nThe following two pages are excellent resources for finding data sets:\n\nUCI Machine Learning Repository\nKaggle\n\n‚ö†Ô∏è Important: Any data set you use needs to be cited."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A First Tutorial on Cluster Analysis",
    "section": "",
    "text": "This page includes some basic material on cluster analysis (clustering).\nThe materials presented were created by Efthymios Costa and they serve as an introduction to dissimilarity-based clustering, involving partitional and hierarchical clustering of continuous data. The intended reader is an undergraduate student who is familiar with basic concepts of statistics and analysis.\nClustering is, roughly speaking, the task of detecting group structures in data sets. It has been used for several applications, such as customer segmentation, gene identification, fraud detection, or document classification, among others. The goal of these materials is to familiarise the reader with some basic clustering concepts and allow them to apply these on some data set that is of interest to them."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "A First Tutorial on Cluster Analysis",
    "section": "",
    "text": "This page includes some basic material on cluster analysis (clustering).\nThe materials presented were created by Efthymios Costa and they serve as an introduction to dissimilarity-based clustering, involving partitional and hierarchical clustering of continuous data. The intended reader is an undergraduate student who is familiar with basic concepts of statistics and analysis.\nClustering is, roughly speaking, the task of detecting group structures in data sets. It has been used for several applications, such as customer segmentation, gene identification, fraud detection, or document classification, among others. The goal of these materials is to familiarise the reader with some basic clustering concepts and allow them to apply these on some data set that is of interest to them."
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "A First Tutorial on Cluster Analysis",
    "section": "Content",
    "text": "Content\nWe will have several sessions/workshops. These will be focusing on different aspects of cluster analysis.\n\nIntroduction: What is clustering?\nDistances & Dissimilarities\nPartitional Clustering\nHierarchical Clustering\nNumber Of Clusters & Evaluation\n\nSome ideas on possible projects are included in Project Ideas, together with links to two data repositories."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "A First Tutorial on Cluster Analysis",
    "section": "Software",
    "text": "Software\nThe tutorials will make use of the R programming language (R Core Team, 2025) and it is recommended that certain packages are installed in advance. Make sure you have downloaded R and RStudio, then go to the Console of RStudio and paste the following:\n\nrequired_pkgs &lt;- c(\"aricode\",\n                   \"cluster\",\n                   \"factoextra\",\n                   \"GGally\",\n                   \"ggplot2\",\n                   \"patchwork\")\ninstall.packages(required_pkgs)\n\nNotice that this may take a bit of time. Additional packages can be used if you wish to use specific functions not available in the packages listed above."
  },
  {
    "objectID": "index.html#reading-list",
    "href": "index.html#reading-list",
    "title": "A First Tutorial on Cluster Analysis",
    "section": "Reading List",
    "text": "Reading List\nThere are several textbooks available that include the materials covered here and much more about clustering. The following provide excellent starting points for exploring clustering in more depth:\n\nKaufman L, Rousseeuw P.J. (1990). Finding Groups in Data: an Introduction to Cluster Analysis. John Wiley & Sons\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning with applications in R [Chapter 12. Unsupervised Learning].\n\n\nHennig, C., Meila, M., Murtagh, F., & Rocci, R. (Eds.). (2015). Handbook of cluster analysis. CRC press."
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "A First Tutorial on Cluster Analysis",
    "section": "Citation",
    "text": "Citation\nIf you use these resources in your work, please cite as follows:\n\nCosta, E. (2026). A First Tutorial on Cluster Analysis [Course materials]. TBC.\n\n@online{costa2026clustering,\n  author = {Efthymios Costa},\n  title = {A {F}irst {T}utorial on {C}luster {A}nalysis},\n  year = {2026},\n  note = {[Course materials]},\n  url = {TBC}\n}"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "A First Tutorial on Cluster Analysis",
    "section": "License",
    "text": "License\nThe content of this website is published under the Creative Commons Attribution 4.0 International license. This license lets you distribute, remix, adapt, and build upon this work, even commercially, on the condition that you give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use."
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "A First Tutorial on Cluster Analysis",
    "section": "About this Website",
    "text": "About this Website\nThis website was built using Quarto. To learn more about Quarto websites visit: https://quarto.org/docs/websites."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "A First Tutorial on Cluster Analysis",
    "section": "References",
    "text": "References\n\n\nR Core Team. (2025). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/"
  },
  {
    "objectID": "quiz/example.html",
    "href": "quiz/example.html",
    "title": "Quiz example",
    "section": "",
    "text": "This document illustrates the usage of the naquiz quarto extension.\nThe extension enables adding multiple choice questions when using HTML documents. It also adds Alpine.js javascript framework to the document."
  },
  {
    "objectID": "quiz/example.html#a-basic-example",
    "href": "quiz/example.html#a-basic-example",
    "title": "Quiz example",
    "section": "A basic example",
    "text": "A basic example\n\nBill Gates was the founder of:\n\n\n\n\n ‚úóApple\n\n\n ‚úìMicrosoft\n\n\n ‚úóFacebook\n\n\n ‚úóGoogle"
  },
  {
    "objectID": "quiz/example.html#a-quiz-with-a-clear-answer-button",
    "href": "quiz/example.html#a-quiz-with-a-clear-answer-button",
    "title": "Quiz example",
    "section": "A quiz with a clear answer button",
    "text": "A quiz with a clear answer button\n\nBill Gates was the founder of:\n\n\n\n\n ‚úóApple\n\n\n ‚úìMicrosoft\n\n\n ‚úóFacebook\n\n\n ‚úóGoogle\n\n\nClear answer"
  },
  {
    "objectID": "quiz/example.html#a-quiz-with-additional-buttons",
    "href": "quiz/example.html#a-quiz-with-additional-buttons",
    "title": "Quiz example",
    "section": "A quiz with additional buttons",
    "text": "A quiz with additional buttons\n\nBill Gates was the founder of:\n\n\n\n\n ‚úóApple\n\n\n ‚úìMicrosoft\n\n\n ‚úóFacebook\n\n\n ‚úóGoogle\n\n\nClear answer\n\n\n\n\n\n\n\n\n\n\n\nShow hint\n\nThe company name starts with an ‚ÄòM‚Äô‚Ä¶\n\n\n\n\n\n\nShow Answer\n\nBill Gates and Paul Allen founded Microsoft on April 4, 1975."
  },
  {
    "objectID": "quiz/example.html#a-quiz-with-additional-information-in-callouts",
    "href": "quiz/example.html#a-quiz-with-additional-information-in-callouts",
    "title": "Quiz example",
    "section": "A quiz with additional information in callouts",
    "text": "A quiz with additional information in callouts\n\nBill Gates was the founder of:\n\n\n\n\n ‚úóApple\n\n\n ‚úìMicrosoft\n\n\n ‚úóFacebook\n\n\n ‚úóGoogle\n\n\nClear answer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\nThe company name starts with an ‚ÄòM‚Äô‚Ä¶\n\n\n\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nBill Gates and Paul Allen founded Microsoft on April 4, 1975."
  },
  {
    "objectID": "quiz/example_presentation.html#a-basic-example",
    "href": "quiz/example_presentation.html#a-basic-example",
    "title": "Quiz example",
    "section": "A basic example",
    "text": "A basic example\n\nBill Gates was the founder of:\n\n\n\n\n ‚úóApple\n\n\n ‚úìMicrosoft\n\n\n ‚úóFacebook\n\n\n ‚úóGoogle"
  },
  {
    "objectID": "quiz/example_presentation.html#a-quiz-with-a-clear-answer-button",
    "href": "quiz/example_presentation.html#a-quiz-with-a-clear-answer-button",
    "title": "Quiz example",
    "section": "A quiz with a clear answer button",
    "text": "A quiz with a clear answer button\n\nBill Gates was the founder of:\n\n\n\n\n ‚úóApple\n\n\n ‚úìMicrosoft\n\n\n ‚úóFacebook\n\n\n ‚úóGoogle\n\n\nClear answer"
  },
  {
    "objectID": "quiz/example_presentation.html#a-quiz-with-additional-buttons",
    "href": "quiz/example_presentation.html#a-quiz-with-additional-buttons",
    "title": "Quiz example",
    "section": "A quiz with additional buttons",
    "text": "A quiz with additional buttons\n\nBill Gates was the founder of:\n\n\n\n\n ‚úóApple\n\n\n ‚úìMicrosoft\n\n\n ‚úóFacebook\n\n\n ‚úóGoogle\n\n\nClear answer\n\n\n\n\n\n\n\n\n\n\nShow hint\n\nThe company name starts with an ‚ÄòM‚Äô‚Ä¶\n\n\n\n\n\n\nShow Answer\n\nBill Gates and Paul Allen founded Microsoft on April 4, 1975."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "0. Introduction: What is clustering?",
    "section": "",
    "text": "If you try to define the term ‚Äúcluster‚Äù, you will often find yourself not being precise enough. Roughly speaking, a cluster is any arbitrary group of observations and a clustering is a collection of clusters, i.e.¬†a partition of objects.\nSuppose we have observations indexed as \\(1, \\ldots, n\\) and a set \\(\\mathcal{C} = \\{ C_1, \\ldots, C_K\\}, K \\in \\mathbb{N}\\). Then, the following need to hold for \\(\\mathcal{C}\\) to be a valid clustering into \\(K\\) clusters:\n\n\\(C_i \\neq \\varnothing \\forall i \\in \\{1, \\ldots, K\\}\\),\n\\(C_i \\cap C_j = \\varnothing \\forall i\\neq j\\),\n\\(\\bigcup\\limits_{i=1}^K C_i = \\{1, \\ldots, n\\}\\).\n\n\n\n\n\n\n\nNoteü§î Quiz: Which of the following is a valid partition of \\(\\{1, \\ldots, 5 \\}\\) into \\(K = 3\\) clusters?\n\n\n\nA. \\(\\mathcal{C} = \\{ \\{1, 2\\}, \\{2, 5\\}, \\{3, 4\\} \\}\\)\nB. \\(\\mathcal{C} = \\{ \\{1, 3\\}, \\{2, 5\\}, \\{4\\} \\}\\)\nC. \\(\\mathcal{C} = \\{ \\{1, 5\\}, \\{\\}, \\{2, 3, 4\\} \\}\\)\nD. \\(\\mathcal{C} = \\{ \\{2\\}, \\{5\\}, \\{3, 4 \\} \\}\\)\n\n\n\nShow Answer\n\n\nüîç Partition Inspector on duty!\nCurrently scanning these cluster candidates for rule violations: no overlaps, no empty sets, every element must belong to exactly one club.\nHint: Some of these ‚Äúpartitions‚Äù are trying to cheat the system!"
  },
  {
    "objectID": "about.html#what-is-clustering",
    "href": "about.html#what-is-clustering",
    "title": "0. Introduction: What is clustering?",
    "section": "",
    "text": "If you try to define the term ‚Äúcluster‚Äù, you will often find yourself not being precise enough. Roughly speaking, a cluster is any arbitrary group of observations and a clustering is a collection of clusters, i.e.¬†a partition of objects.\nSuppose we have observations indexed as \\(1, \\ldots, n\\) and a set \\(\\mathcal{C} = \\{ C_1, \\ldots, C_K\\}, K \\in \\mathbb{N}\\). Then, the following need to hold for \\(\\mathcal{C}\\) to be a valid clustering into \\(K\\) clusters:\n\n\\(C_i \\neq \\varnothing \\forall i \\in \\{1, \\ldots, K\\}\\),\n\\(C_i \\cap C_j = \\varnothing \\forall i\\neq j\\),\n\\(\\bigcup\\limits_{i=1}^K C_i = \\{1, \\ldots, n\\}\\).\n\n\n\n\n\n\n\nNoteü§î Quiz: Which of the following is a valid partition of \\(\\{1, \\ldots, 5 \\}\\) into \\(K = 3\\) clusters?\n\n\n\nA. \\(\\mathcal{C} = \\{ \\{1, 2\\}, \\{2, 5\\}, \\{3, 4\\} \\}\\)\nB. \\(\\mathcal{C} = \\{ \\{1, 3\\}, \\{2, 5\\}, \\{4\\} \\}\\)\nC. \\(\\mathcal{C} = \\{ \\{1, 5\\}, \\{\\}, \\{2, 3, 4\\} \\}\\)\nD. \\(\\mathcal{C} = \\{ \\{2\\}, \\{5\\}, \\{3, 4 \\} \\}\\)\n\n\n\nShow Answer\n\n\nüîç Partition Inspector on duty!\nCurrently scanning these cluster candidates for rule violations: no overlaps, no empty sets, every element must belong to exactly one club.\nHint: Some of these ‚Äúpartitions‚Äù are trying to cheat the system!"
  },
  {
    "objectID": "about.html#does-any-partition-count",
    "href": "about.html#does-any-partition-count",
    "title": "0. Introduction: What is clustering?",
    "section": "Does any partition count?",
    "text": "Does any partition count?\nAny partition of our observations can be considered a valid clustering. So is there a way to guide the process and assign objects into groups so that they are meaningful? Turns out that this is possible, as long as we define some desirable characteristics or properties that we want our groups to possess. A very insightful (and rather philosophical) paper that summarises such properties is Hennig (2015). Briefly, some things we typically wish to have are:\n\nHighly similar objects are assigned into the same cluster.\nHighly dissimilar objects are assigned into distinct clusters.\nClusters are stable, so that re-clustering should not produce a different group structure.\nThere are not too many clusters, effectively summarising different modes in the data.\nClusters can be well-represented by observations that are central to each group.\n\nThere are many more characteristics that one may wish to get; when developing a clustering algorithm, it is always important to have your ultimate goal as your guide."
  },
  {
    "objectID": "about.html#clustering-in-a-nutshell",
    "href": "about.html#clustering-in-a-nutshell",
    "title": "0. Introduction: What is clustering?",
    "section": "Clustering in a nutshell",
    "text": "Clustering in a nutshell\n\n\n\nClustering in a nutshell - Source: Author‚Äôs own work\n\n\nThe comic illustration above shows how clustering typically works; similar observations are assigned in the same cluster, whereas dissimilar items are in distinct clusters. How do we compute similarities between objects though? This is what we are exploring in more detail in 1. Distances & Dissimilarities."
  },
  {
    "objectID": "about.html#references",
    "href": "about.html#references",
    "title": "0. Introduction: What is clustering?",
    "section": "References",
    "text": "References\n\n\nHennig, C. (2015). What are the true clusters? Pattern Recognition Letters, 64, 53‚Äì62."
  },
  {
    "objectID": "evalmetrics.html",
    "href": "evalmetrics.html",
    "title": "4. Number Of Clusters & Evaluation",
    "section": "",
    "text": "One of the most challenging problems in cluster analysis is estimating the number of clusters \\(K\\). Several ideas have been proposed in the literature; we present some of the most commonly used ones here."
  },
  {
    "objectID": "evalmetrics.html#the-elbow-method",
    "href": "evalmetrics.html#the-elbow-method",
    "title": "4. Number Of Clusters & Evaluation",
    "section": "The Elbow Method",
    "text": "The Elbow Method\nLet us take the iris data set and perform K-means clustering with \\(K = 2, 3, \\ldots, 8\\) and record the within-cluster sum of squares for each clustering solution.\n\ndata(iris)\n# Remove species variables\niris &lt;- iris[, -5]\n\n# Initialise a vector to store the within-cluster sums of squares\nwcss &lt;- c()\n\n# Run K-Means with K = 2, 3, ..., 8\nfor (K in c(2:8)){\n  kmeans_res &lt;- kmeans(x = iris,\n                       centers = K,\n                       iter.max = 100,\n                       nstart = 100)\n  # Store within-cluster sum of squares\n  wcss &lt;- c(wcss, kmeans_res$tot.withinss)\n}\n\n# Plot of the within-cluster sum of squares values against K\nplot(x = c(2:8), y = wcss,\n     type = 'b', lwd = 2,\n     pch = 16,\n     xlab = 'K',\n     ylab = 'Within-cluster sum of squares',\n     main = 'Within-cluster sum of squares against number of clusters (K) on iris')\n\n\n\n\n\n\n\n\nWe observe a decreasing trend of the within-cluster sum of squares as \\(K\\) increases. The elbow heuristic suggests selecting the value of \\(K\\) corresponding to the ‚Äúelbow‚Äù of the curve. The elbow of a curve is, roughly speaking, the point where the curve visibly bends from high to low slope to low slope. On the iris data set, the elbow point corresponds to \\(K=3\\), suggesting that 3 clusters are most appropriate.\n\nüí° Fun fact\nIf a curve is increasing, the analogous to an elbow point is called the knee of the curve.\n\n\n‚ùì Why does this work?\nThe within-cluster sum of squares always decreases as \\(K\\) increases. Can you explain why? Why is the elbow point of interest here? (Hint: Think of what it means for the WCSS to drop sharply when going from \\(K\\) to \\(K+1\\) clusters and then decreasing very slowly)."
  },
  {
    "objectID": "evalmetrics.html#intrinsic-evaluation-metrics",
    "href": "evalmetrics.html#intrinsic-evaluation-metrics",
    "title": "4. Number Of Clusters & Evaluation",
    "section": "Intrinsic Evaluation Metrics",
    "text": "Intrinsic Evaluation Metrics\nThe elbow method is commonly criticised in the literature for several reasons, such as:\n\nHigh ambiguity; what if there is no clearly visible elbow?\nSubjectivity; it is up to the user to decide where the elbow is.\nManual inspection; the process requires user intervention, i.e.¬†it is not automated.\n\nThe need for a quantitative and more theoretically-grounded approach to selecting the number of clusters has led to the formulation of several intrinsic evaluation metrics which determine the quality of a partition by assessing some criteria. We present some of these in the Table below:\n\n\n\nMetric\nRange\nHigher is Better?\n\n\n\n\nSilhouette Coefficient\n\\([-1, 1]\\)\n‚úÖ Yes\n\n\nGap Statistic\n\\((-\\infty, \\infty)\\)\n‚úÖ Yes\n\n\nCalinski-Harabasz Index\n\\([0, \\infty)\\)\n‚úÖ Yes\n\n\nDavies-Bouldin Index\n\\([0, \\infty)\\)\n‚ùå No\n\n\nDunn Index\n\\([0, \\infty)\\)\n‚úÖ Yes\n\n\n\nThere exist many more intrinsic evaluation measures; a detailed description can be found in Charrad et al. (2014) and these are implemented in the NbClust package in R. We describe two of these metrics below."
  },
  {
    "objectID": "evalmetrics.html#silhouette-coefficient",
    "href": "evalmetrics.html#silhouette-coefficient",
    "title": "4. Number Of Clusters & Evaluation",
    "section": "Silhouette Coefficient",
    "text": "Silhouette Coefficient\nThe elbow method takes into consideration the within-cluster sum of squares. Therefore, it only accounts for compactness of the clusters. However, this tells us nothing about how well-separated the clusters are. The silhouette coefficient circumvents this shortcoming of the elbow and produces values in the range \\([-1, 1]\\) that tell us how similar an observations is to its own cluster compared to other clusters. Assuming we run a clustering algorithm assuming \\(K\\) clusters and \\(i \\in C_k\\), that is, \\(\\mathbf{x}_{i}\\) is assigned to cluster \\(1 \\leq k \\leq K\\), the silhouette of observation \\(i\\) is given by:\n\\[\ns(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},\n\\]\nwhere: \\[\n\\begin{align*}\na(i) & = \\frac{1}{\\lvert C_k \\rvert - 1}\\sum\\limits_{i' \\in C_k, i' \\neq i} d(\\mathbf{x}_i, \\mathbf{x}_{i'}), \\\\\nb(i) & = \\min\\limits_{i' \\neq i} \\frac{1}{\\lvert C_{k'}\\rvert} \\sum\\limits_{i' \\in C_{k'}} d(\\mathbf{x}_i, \\mathbf{x}_{i'}),\n\\end{align*}\n\\]\nwith \\(d\\) being the dissimilarity function used for clustering and denoting the cluster that observation \\(\\mathbf{x}_{i'}\\) is assigned to by \\(C_{k'}\\).\n\n‚ùì What do \\(a(i)\\) and \\(b(i)\\) represent?\nCan you explain \\(a(i)\\) and \\(b(i)\\) in simple words?\n\nWe can compute the silhouette for all \\(n\\) observations; the final decision is then based on the average of these silhouette values, also known as the average silhouette width.\nThe cluster package allows us to visualise all silhouette values of a data set. For instance, for the iris data set and K-Means with \\(K = 3\\):\n\nlibrary(cluster)\n\n# Run k-Means on iris with K = 3\nkmeans_res &lt;- kmeans(x = iris,\n                     centers = 3,\n                     iter.max = 100,\n                     nstart = 100)\n\n# Compute silhouette widths\n# Note that x must be the cluster allocation and dist is the dissimilarity matrix\nsilhouette_vals &lt;- silhouette(x = kmeans_res$cluster,\n                              dist = dist(iris))\n\n# Plot the silhouette values\nplot(silhouette_vals,\n     col = c('red', 'blue', 'forestgreen'),\n     main = 'Silhouette widths for K-Means clustering on iris')\n\n\n\n\n\n\n\n\nIf you want to compute the average silhouette value for several values of \\(K\\) and then decide the number of clusters based on the value of \\(K\\) that maximises the average silhouette, the fviz_nbclust function from the factoextra package does exactly that and also visualises the results!\n\nlibrary(factoextra)\n\n# Use the fviz_nbclust function to compute and visualise avg. silhouette widths\nfviz_nbclust(x = iris,\n             FUNcluster = kmeans,\n             method = \"silhouette\",\n             k.max = 10,\n             linecolor = 'blue') +\n  ggtitle(\"Average silhouette width of K-Means solutions on iris data set\")\n\n\n\n\n\n\n\n\nInterestingly, this suggests that the optimal number of clusters in iris is \\(K=2\\) and not \\(K=3\\) that we previously got using the elbow method!\nIt is generally recommended to use several intrinsic evaluation measures to decide the number of clusters. The silhouette may hint the presence of two clusters but other metrics may all agree on \\(K=3\\) for instance."
  },
  {
    "objectID": "evalmetrics.html#gap-statistic",
    "href": "evalmetrics.html#gap-statistic",
    "title": "4. Number Of Clusters & Evaluation",
    "section": "Gap Statistic",
    "text": "Gap Statistic\nThe Gap statistic is a more statistically principled way of estimating the number of clusters. It is based on the idea that if a cluster structure exists in a data set, the clusters will be significantly more compact than what you would expect from randomly distributed data.\nThe following steps are used for computing the Gap statistic:\n\nCompute WCSS for data set:\n\nPerform clustering on the data set assuming \\(K\\) clusters.\nCompute the within-cluster sum of squares \\(W_k\\).\n\nGenerate reference data sets:\n\nCreate \\(B\\) uniformly random data sets with the same dimensions as your data.\nFor each feature, sample values uniformly between the observed minimum and maximum values.\nThese reference data sets have no inherent cluster structure.\n\nCompute WCSS for each reference dataset:\n\nFor each of the \\(B\\) random data sets \\(b = 1, \\dots, B\\):\n\nPerform clustering assuming \\(K\\) clusters.\nCompute reference WCSS \\(W_{kb}^*\\).\n\n\nCalculate the Gap Statistic: \\[\n\\text{Gap}(K) = \\frac{1}{B} \\sum_{b=1}^{B} \\log(W_{kb}^*) - \\log(W_k)\n\\] Where:\n\n\\(\\log(W_k)\\) = log of actual WCSS,\n\\(\\log(W_{kb}^*)\\) = log of reference WCSS for data set \\(b\\).\n\nCalculate standard deviation: \\[\n\\text{sd}(K) = \\sqrt{\\frac{1}{B} \\sum_{b=1}^{B} \\left[\\log(W_{kb}^*) - \\frac{1}{B} \\sum_{b=1}^{B} \\log(W_{kb}^*)\\right]^2}\n\\]\n\nThe above steps are repeated for several \\(K\\) values. The number of clusters is selected to be either the value of \\(K\\) for which \\(\\text{Gap}(K)\\) is maximised, or the minimum \\(K\\) value for which \\(\\text{Gap}(K) \\geq \\text{Gap}(K+1) ‚àí \\text{sd}(K+1)\\).\nGood news is you do not need to compute the Gap statistic manually (although this would be a fun coding exercise). The clusGap function from the cluster package provides a computation of the Gap statistic for a range of \\(K\\) values. We use this to compute the Gap statistic for \\(K = 1, \\ldots, 10\\) on the iris data set with \\(B=100\\) (i.e.¬†100 reference data sets for each value of \\(K\\)). We can then visualise the results using the fviz_gap_stat function from factoextra package. The fviz_gap_stat function also allows for determining the rule for selecting the number of clusters (e.g.¬†maximum Gap value, or the one standard error rule).\n\nlibrary(patchwork)\n\n# Compute Gap statistic values\niris_gap &lt;- clusGap(x = iris,\n                    FUNcluster = kmeans,\n                    nstart = 100,\n                    K.max = 10,\n                    B = 100,\n                    verbose = FALSE)\n\n# Use the fviz_gap_stat function to visualise Gap statistic values\n# Select the number of clusters using the maximum Gap\np1 &lt;- fviz_gap_stat(gap_stat = iris_gap,\n                    linecolor = 'red',\n                    maxSE = list(method = \"globalmax\")) +\n  labs(title = \"Gap statistic values for K-Means solutions on iris data set\",\n       subtitle = \"(Maximum Gap)\")\n# Select the number of clusters using the one SE rule\np2 &lt;- fviz_gap_stat(gap_stat = iris_gap,\n                    linecolor = 'red',\n                    maxSE = list(method = \"Tibs2001SEmax\")) +\n  labs(title = \"Gap statistic values for K-Means solutions on iris data set\",\n       subtitle = \"(One SE rule)\")\n\np1 / p2\n\n\n\n\n\n\n\n\nWe see that the two rules give two completely different numbers of clusters. At the end, it is up to us to select the number of clusters and this may be based on the context, domain knowledge, or simply what the different solutions in varying numbers of clusters mean!"
  },
  {
    "objectID": "evalmetrics.html#extrinsic-evaluation-metrics",
    "href": "evalmetrics.html#extrinsic-evaluation-metrics",
    "title": "4. Number Of Clusters & Evaluation",
    "section": "Extrinsic Evaluation Metrics",
    "text": "Extrinsic Evaluation Metrics\nSuppose we have run clustering on a data set using two different clustering algorithms and we want to check how similar these solutions are. Or perhaps we have cluster labels and wish to assess how much our obtained partition agrees with the labels. In this case, we need to use extrinsic evaluation metrics to quantify this agreement! We provide a list of some extrinsic evaluation metrics below (you do not need to know or use all of them; usually just 2-3 are enough):\n\n\n\n\n\n\n\n\nMetric\nRange\nHigher is Better?\n\n\n\n\nRand Index (RI)\n\\([0, 1]\\)\n‚úÖ Yes\n\n\nAdjusted Rand Index (ARI)\n\\([-1, 1]\\)\n‚úÖ Yes\n\n\nFowlkes-Mallows Index (FMI)\n\\([0, 1]\\)\n‚úÖ Yes\n\n\nJaccard Index\n\\([0, 1]\\)\n‚úÖ Yes\n\n\nMutual Information (MI)\n\\([0, \\infty)\\)\n‚úÖ Yes\n\n\nAdjusted Mutual Information (AMI)\n\\([-1, 1]\\)\n‚úÖ Yes\n\n\nNormalised Mutual Information (NMI)\n\\([0, 1]\\)\n‚úÖ Yes\n\n\nHomogeneity\n\\([0, 1]\\)\n‚úÖ Yes\n\n\nCompleteness\n\\([0, 1]\\)\n‚úÖ Yes\n\n\nV-measure\n\\([0, 1]\\)\n‚úÖ Yes\n\n\nPurity\n\\([0, 1]\\)\n‚úÖ Yes\n\n\n\n\nüí° Fun fact\nExtrinsic evaluation metrics can also compare two partitions with different numbers of clusters!"
  },
  {
    "objectID": "evalmetrics.html#adjusted-rand-index",
    "href": "evalmetrics.html#adjusted-rand-index",
    "title": "4. Number Of Clusters & Evaluation",
    "section": "(Adjusted) Rand Index",
    "text": "(Adjusted) Rand Index\nThe Rand Index (RI) is one of the most commonly used extrinsic evaluation metrics in clustering. In order to define the RI, we assume that we have access to two partitions \\(\\mathcal{C}_1\\) and \\(\\mathcal{C}_2\\) of the same \\(n\\) objects into \\(K_1\\) and \\(K_2\\) clusters, respectively (note that it is possible that \\(K_1 \\neq K_2\\)). We introduce the following notation:\n\n\\(a\\): Number of object pairs which are in the same cluster in \\(\\mathcal{C}_1\\) and in the same cluster in \\(\\mathcal{C}_2\\).\n\\(b\\): Number of object pairs which are in distinct clusters in \\(\\mathcal{C}_1\\) and in distinct clusters in \\(\\mathcal{C}_2\\).\n\\(c\\): Number of object pairs which are in the same cluster in \\(\\mathcal{C}_1\\) and in distinct clusters in \\(\\mathcal{C}_2\\).\n\\(d\\): Number of object pairs which are in distinct clusters in \\(\\mathcal{C}_1\\) and in the same cluster in \\(\\mathcal{C}_2\\).\n\nThe RI between \\(\\mathcal{C}_1\\) and \\(\\mathcal{C}_2\\) is then defined as: \\[\n\\text{RI}(\\mathcal{C}_1, \\mathcal{C}_2) = \\frac{a + b}{a + b + c + d} = \\frac{a + b}{\\binom{n}{2}}.\n\\]\n\n‚ùì Why does the denominator in the RI definition simplify?\nCan you explain why the denominator \\(a + b + c + d\\) in the above definition of the RI simplifies to \\(\\binom{n}{2}\\)?\n\n\n\n\n\n\n\nNoteü§î Quiz: Let \\(n = 5\\) and suppose we have two partitions \\(\\mathcal{C}_1 = \\{ \\{1, 2, 3\\}, \\{4, 5 \\} \\}\\) and \\(\\mathcal{C}_2 = \\{ \\{4, 1, 5\\}, \\{2, 3 \\} \\}\\). What is the value of \\(\\text{RI}(\\mathcal{C}_1, \\mathcal{C}_2)\\)?\n\n\n\nA. \\(0.3\\)\nB. \\(0.4\\)\nC. \\(0.5\\)\nD. \\(0.6\\)\n\n\n\nShow Answer\n\n\nüé≤ The RI is currently being audited‚Ä¶\nDid these partitions agree more than two random clusterings would?\nOur statisticians are still debating whether this similarity is significant or just randomly significant!\n\n\n\nThe Adjusted Rand Index (ARI) is the adjusted-for-chance version of the RI. It takes into account the expected similarity among all pairwise comparisons between two clusterings specified by a random model and corrects the RI accordingly. Suppose we have the following cross-tabulation of two partitions \\(\\mathcal{C}_1\\) and \\(\\mathcal{C}'\\):\n\n\n\n\n\\(C'_1\\)\n\\(\\ldots\\)\n\\(C'_{K'}\\)\nSum\n\n\n\n\n\\(C_1\\)\n\\(n_{1,1}\\)\n\\(\\ldots\\)\n\\(n_{1,K'}\\)\n\\(a_1\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(C_{K}\\)\n\\(n_{K,1}\\)\n\\(\\ldots\\)\n\\(n_{K, K'}\\)\n\\(a_{K}\\)\n\n\nSum\n\\(b_1\\)\n\\(\\ldots\\)\n\\(b_{K'}\\)\n\\(n\\)\n\n\n\nwhere \\(C_k\\) and \\(C'_k\\) denote the \\(k\\)th cluster in \\(\\mathcal{C}\\) and \\(\\mathcal{C}'\\), respectively, while \\(K\\) and \\(K'\\) are the number of clusters that the two partitions assume. A hypergeometric model is used as a baseline random model, according to which: \\[\nn_{i,j} \\sim \\text{Hypergeometric}(n, a_i, b_j),\n\\]\nwith \\(n_{i, j}\\) representing the number of objects assigned in clusters \\(C_i\\) and \\(C'_{j}\\).\n\n‚ùì How can you explain this?\nCan you give an interpretation of the use of the hypergeometric distribution for the random model?\n\nTherefore, the ARI between \\(\\mathcal{C}\\) and \\(\\mathcal{C}'\\) is given by:\n\\[\n\\begin{align*}\n    \\text{ARI}(\\mathcal{C}, \\mathcal{C}') &= \\frac{\\text{RI}(\\mathcal{C}, \\mathcal{C}') - \\mathbb{E}\\left[ \\text{RI}(\\mathcal{C}, \\mathcal{C}') \\right]}{\\max \\text{RI}(\\mathcal{C}, \\mathcal{C}') - \\mathbb{E}\\left[ \\text{RI}(\\mathcal{C}, \\mathcal{C}') \\right]]} \\\\\n    & = \\frac{\\sum\\limits_{i=1}^K \\sum\\limits_{j=1}^{K'} \\binom{n_{i,j}}{2} - \\frac{\\sum\\limits_{i=1}^K \\binom{a_i}{2}\\sum\\limits_{j=1}^{K'}\\binom{b_j}{2}}{\\binom{n}{2}}}{\\frac{1}{2}\\left\\{ \\sum\\limits_{i=1}^K \\binom{a_i}{2} + \\sum\\limits_{j=1}^{K'}\\binom{b_j}{2} \\right\\} - \\frac{\\sum\\limits_{i=1}^K \\binom{a_i}{2}\\sum\\limits_{j=1}^{K'}\\binom{b_j}{2}}{\\binom{n}{2}}}.\n\\end{align*}\n\\]\nThe Expression for the ARI may look a bit complicated but there is no need to compute it manually! The aricode package in R includes relevant functions for computing the RI, the ARI, as well as some other extrinsic evaluation metrics. The example below is an illustration of how to use these functions on the iris data set, assuming we have run K-Means with \\(K=3\\) and we wish to compare our clusters to the actual labels (i.e.¬†the Species feature that we previously removed).\n\nlibrary(aricode)\n\n# Reload the data to get access to the Species feature\ndata(iris)\nspecies &lt;- iris$Species\n\n# Now remove the labels to run clustering on the rest of the features\niris &lt;- iris[, -5]\nkmeans_res &lt;- kmeans(x = iris,\n                     centers = 3,\n                     iter.max = 100,\n                     nstart = 100)\n\n# Compute the RI and the ARI\nri_iris &lt;- RI(kmeans_res$cluster, species)\nari_iris &lt;- ARI(kmeans_res$cluster, species)\n\ncat('Rand Index:', ri_iris, '\\n')\n\nRand Index: 0.8797315 \n\ncat('Adjusted Rand Index:', ari_iris, '\\n')\n\nAdjusted Rand Index: 0.7302383 \n\n\n\n‚ùì Negative ARI values\nThe RI is restricted to give a value in the unit interval \\([0, 1]\\). In contrast, the ARI produces values in \\([-1, 1]\\) instead. How would you interpret a negative value of the ARI?\n\nNote that it is always a good idea to report several evaluation metrics. The aricode package provides implementations for some of these, such as the AMI and the NMI. Unfortunately, there is no R package that includes all of the extrinsic evaluation metrics mentioned here; you are more than welcome to use functions from other packages that calculate these extrinsic evaluation metrics."
  },
  {
    "objectID": "evalmetrics.html#visualisations",
    "href": "evalmetrics.html#visualisations",
    "title": "4. Number Of Clusters & Evaluation",
    "section": "Visualisations",
    "text": "Visualisations\nClustering is an unsupervised learning problem, therefore there is no ‚Äúground truth‚Äù that is readily available. When it comes to evaluating our clustering output, it is usually best to refer to a qualitative assessment of the resulting partition.\nThere are several ways one can visualise the resulting output. These allow for a qualitative assessment of our clusters. We will make use of two such plots, namely pair plots and parallel coordinate plots. A pair plot is simply a matrix of pairwise scatter plots of the observations; that is, a visualisation of observations on all pairs of variables. Pair plots can get extremely large when the number of features included in the data is very large. Below, we provide an example of a pair plot on the iris data; we use the obtained partition into 3 clusters as our solution.\n\n# Take the full iris data set and add a column for the clustering solution\ndata(iris)\niris$Cluster &lt;- as.factor(kmeans_res$cluster)\n\n# Plot variables where colour = obtained cluster label + point shape = Species\npairs(iris[, c(1:4)],\n      col = iris$Cluster,\n      pch = as.integer(iris$Species))\n\n\n\n\n\n\n\n\nThe GGally package provides some nice pair plot visualisations including additional information using the ggpairs function. This also produces a legend that can be used for reference.\n\nlibrary(ggplot2)\nlibrary(GGally)\n\nggpairs(data = iris,\n        columns = c(1:4),\n        aes(col = Cluster, shape = Species),\n        legend = c(2, 1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nüí° Fun fact\nThe ggpairs function returns more than just the pair plots! It also reports the overall correlation between variables and correlations between variables for observations in each cluster (can you explain what the asterisks mean?), as well as density estimates of the features (on the diagonal) for each cluster.\n\nFor instance, looking at these plots shows that the versicolor and virginica species are overlapping and this has confused K-Means. This is perhaps why the Silhouette was suggesting two clusters.\n\n‚ùì Solution suggested by silhouette\nThe Silhouette method suggested a solution into 2 clusters. Try running K-Means with \\(K=2\\) on the iris data set and create a pairs plot. What do you see? Can you justify why the silhouette was suggesting 2 clusters instead of 3?\n\nA second type of plots that is usually of interest is the parallel coordinates plot. This is simply a plot where a statistic of interest is visualised for each variable, with different lines representing different clusters. The function below can be used for generating a parallel coordinates plot and we use it on our K-Means solution on iris.\n\nlibrary(ggplot2)\n\nparallel_coords_plot &lt;- function(data, labels, stat_fun = mean, \n                                 rotate_labels = 45, \n                                 line_size = 1.2,\n                                 point_size = 3) {\n  \n  data &lt;- as.data.frame(data)\n  clusters &lt;- unique(labels)\n  n_clusters &lt;- length(clusters)\n  n_features &lt;- ncol(data)\n  feature_names &lt;- colnames(data)\n  stat_matrix &lt;- matrix(NA, nrow = n_clusters, ncol = n_features)\n  \n  for(i in seq_along(clusters)) {\n    cluster_data &lt;- data[labels == clusters[i], , drop = FALSE]\n    for(j in 1:n_features) {\n      stat_matrix[i, j] &lt;- stat_fun(cluster_data[, j])\n    }\n  }\n  \n  plot_data &lt;- data.frame(\n    Feature = rep(feature_names, each = n_clusters),\n    Value = as.vector(t(stat_matrix)),\n    Cluster = rep(as.character(clusters), times = n_features),\n    Feature_num = rep(1:n_features, each = n_clusters)\n  )\n  p &lt;- ggplot(plot_data, aes(x = Feature_num, y = Value, \n                             group = Cluster, color = Cluster)) +\n    geom_line(linewidth = line_size) +\n    geom_point(size = point_size) +\n    scale_x_continuous(breaks = 1:n_features, labels = feature_names) +\n    theme_bw() +\n    labs(x = \"Features\", y = paste0(\"Statistic (\", deparse(substitute(stat_fun)), \")\"),\n         color = \"Cluster\") +\n    theme(\n      axis.text.x = element_text(angle = rotate_labels, \n                                 hjust = if(rotate_labels &gt; 0) 1 else 0.5, \n                                 size = 10),\n      legend.position = \"right\",\n      panel.grid.major.x = element_line(color = \"grey80\"),\n      panel.grid.minor = element_blank()\n    )\n  \n  return(p)\n}\n\n# Example usage with the K-Means labels\nparallel_coords_plot(data = iris[, c(1:4)],\n                     labels = kmeans_res$cluster,\n                     stat_fun = mean,\n                     rotate_labels = 45) +\n  labs(title = 'Parallel coordinates plot of K-Means solution on iris data')\n\n\n\n\n\n\n\n\nThus, we can see that one of the clusters stands out as it includes flowers with large average petal length, whereas some other cluster contains flowers with very small sepal widths. We can also use a parallel coordinates plot on the actual species labels to have a much more complete picture.\n\n# Parallel coordinates plot using Species labels\nparallel_coords_plot(data = iris[, c(1:4)],\n                     labels = iris$Species,\n                     stat_fun = mean,\n                     rotate_labels = 45) +\n  labs(title = 'Parallel coordinates plot of iris Species')\n\n\n\n\n\n\n\n\n\nüöÄ Time to practice!\nThe USArrests data set includes statistics for assault, murder, and rape in each of the 50 US states from 1973. Load the USArrests data in R (just use data(USArrests) to load it to your working environment). Use a clustering algorithm of your choice and try to estimate the number of clusters using different intrinsic evaluation metrics. Then, use a second clustering algorithm, estimate the number of clusters and compare the partitions each algorithm has given you using several extrinsic evaluation metrics. Visualise both obtained partitions and try interpreting them. Which one makes more sense to you and why?"
  },
  {
    "objectID": "evalmetrics.html#key-takeaways",
    "href": "evalmetrics.html#key-takeaways",
    "title": "4. Number Of Clusters & Evaluation",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nEstimating the number of clusters is a challenging problem in clustering.\nThe Elbow method is a heuristic that can be used for estimating the number of clusters.\nIntrinsic evaluation measures (such as the Silhouette, or the Gap statistic) provide a more principled approach to the problem.\nExtrinsic evaluation metrics quantify the agreement between two partitions or between a partition and cluster labels (if those exist).\nInterpreting the clustering output requires a qualitative assessment of the clusters; this is why visualisations using e.g.¬†pair plots or parallel coordinate plots are very useful."
  },
  {
    "objectID": "dists.html",
    "href": "dists.html",
    "title": "1. Distances & Dissimilarities",
    "section": "",
    "text": "Definition (Distance Function/Metric): A distance function, also known as a metric, is a function \\(d: M \\times M \\rightarrow \\mathbb{R}\\), where \\(M\\) is a set, satisfying the following:\n\n\\(d(x, y) \\geq 0 \\forall x, y \\in M\\) (Non-negativity),\n\\(d(x,y) = d(y, x) \\forall x,y \\in M\\) (Symmetry),\n\\(d(x, y) = 0 \\iff x = y\\) (Indiscernibles),\n\\(d(x, z) \\leq d(x, y) + d(y, z) \\forall x,y,z \\in M\\) (Triangle inequality)"
  },
  {
    "objectID": "dists.html#what-is-a-distance",
    "href": "dists.html#what-is-a-distance",
    "title": "1. Distances & Dissimilarities",
    "section": "",
    "text": "Definition (Distance Function/Metric): A distance function, also known as a metric, is a function \\(d: M \\times M \\rightarrow \\mathbb{R}\\), where \\(M\\) is a set, satisfying the following:\n\n\\(d(x, y) \\geq 0 \\forall x, y \\in M\\) (Non-negativity),\n\\(d(x,y) = d(y, x) \\forall x,y \\in M\\) (Symmetry),\n\\(d(x, y) = 0 \\iff x = y\\) (Indiscernibles),\n\\(d(x, z) \\leq d(x, y) + d(y, z) \\forall x,y,z \\in M\\) (Triangle inequality)"
  },
  {
    "objectID": "dists.html#the-euclidean-distance",
    "href": "dists.html#the-euclidean-distance",
    "title": "1. Distances & Dissimilarities",
    "section": "The Euclidean distance",
    "text": "The Euclidean distance\nDefinition (Euclidean distance on \\(\\mathbb{R}^d\\)): The Euclidean distance between two points \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d\\) is defined as follows: \\[\nd_E(\\mathbf{x}, \\mathbf{y}) = \\| \\mathbf{x} - \\mathbf{y} \\| = \\sqrt{\\sum\\limits_{j=1}^d (x_j - y_j)^2}.\n\\] The Euclidean distance is one of the most basic and most useful distance functions that you are probably already familiar with. It can be calculated in any finite dimension \\(p\\) and has a very intuitive interpretation as the length of the line segment between any two observations.\nExample: The Euclidean distance on \\(\\mathbb{R}^d\\) is a valid distance function. Let‚Äôs prove this:\n1. Non-negativity:\nSince \\((x_j - y_j)^2 \\geq 0\\) for all \\(j\\), we have \\(\\sum_{j=1}^d (x_j - y_j)^2 \\geq 0\\). Taking the square root preserves non-negativity, so \\(d_E(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{j=1}^d (x_j - y_j)^2} \\geq 0\\).\n2. Symmetry:\n\\[d_E(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{j=1}^d (x_j - y_j)^2} = \\sqrt{\\sum_{j=1}^d (y_j - x_j)^2} = d_E(\\mathbf{y}, \\mathbf{x}),\\]\nsince \\((x_j - y_j)^2 = (y_j - x_j)^2\\).\n3. Identity of indiscernibles:\n\\((\\Rightarrow)\\) If \\(d_E(\\mathbf{x}, \\mathbf{y}) = 0\\), then \\(\\sqrt{\\sum_{j=1}^d (x_j - y_j)^2} = 0\\), which implies \\(\\sum_{j=1}^d (x_j - y_j)^2 = 0\\). Since each term \\((x_j - y_j)^2 \\geq 0\\) and their sum is zero, we must have \\((x_j - y_j)^2 = 0\\) for all \\(j\\), therefore \\(x_j = y_j \\forall j \\in \\{1, \\ldots, d\\}\\), so \\(\\mathbf{x} = \\mathbf{y}\\).\n\\((\\Leftarrow)\\) If \\(\\mathbf{x} = \\mathbf{y}\\), then \\(x_j = y_j\\) for all \\(j\\), so \\(d_E(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{j=1}^d 0^2} = 0\\).\n4. Triangle inequality:\nWe need to show \\(d_E(\\mathbf{x}, \\mathbf{z}) \\leq d_E(\\mathbf{x}, \\mathbf{y}) + d_E(\\mathbf{y}, \\mathbf{z})\\), or equivalently: \\[\\|\\mathbf{x} - \\mathbf{z}\\| \\leq \\|\\mathbf{x} - \\mathbf{y}\\| + \\|\\mathbf{y} - \\mathbf{z}\\|.\\]\nNote that \\(\\mathbf{x} - \\mathbf{z} = (\\mathbf{x} - \\mathbf{y}) + (\\mathbf{y} - \\mathbf{z})\\). By the Cauchy-Schwarz inequality and properties of norms in \\(\\mathbb{R}^d\\), we have: \\[\\|\\mathbf{x} - \\mathbf{z}\\| = \\|(\\mathbf{x} - \\mathbf{y}) + (\\mathbf{y} - \\mathbf{z})\\| \\leq \\|\\mathbf{x} - \\mathbf{y}\\| + \\|\\mathbf{y} - \\mathbf{z}\\|.\\]\nThis is the standard triangle inequality for the Euclidean norm. \\(\\square\\)"
  },
  {
    "objectID": "dists.html#the-manhattan-distance",
    "href": "dists.html#the-manhattan-distance",
    "title": "1. Distances & Dissimilarities",
    "section": "The Manhattan distance",
    "text": "The Manhattan distance\nDefinition (Manhattan distance on \\(\\mathbb{R}^d\\)): The Manhattan distance (also known as taxicab, city block, or rectilinear distance) between two points \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d\\) is defined as follows: \\[\nd_T(\\mathbf{x}, \\mathbf{y}) = \\| \\mathbf{x} - \\mathbf{y} \\|_T = \\sum\\limits_{j=1}^d \\lvert x_j - y_j\\rvert.\n\\] The Manhattan distance makes use of the so-called Manhattan geometry, where the distance between any two points is defined as the sum of the absolute differences of their Cartesian coordinates.\n\nüí° Fun fact\nThe reason it‚Äôs called the ‚ÄúManhattan‚Äù, ‚Äútaxicab‚Äù, or ‚Äúcity block‚Äù distance is because of the famous borough of Manhattan in New York City, which is planned with a rectangular grid of streets, meaning that a taxicab can only travel along grid directions.\n\n\n\n\n\n\n\nNoteü§î Quiz: Which of the following line colours in the Figure illustrate the Euclidean distance between \\((0, 0)\\) and \\((6, 6)\\) and which illustrate the Manhattan distance?\n\n\n\n\n\n\n\n\nA. Euclidean: Yellow, Manhattan: Red, Blue, Green\nB. Euclidean: Blue, Red, Manhattan: Yellow, Green\nC. Euclidean: Blue, Red, Yellow Manhattan: Green\nD. Euclidean: Green, Manhattan: Blue, Red Yellow\n\n\n\nShow Answer\n\n\nError 404: Answer not found.\nBut seriously, think about it this way; if each square is a building and the lines are the roads, which of these routes could a taxi take? üöï"
  },
  {
    "objectID": "dists.html#the-minkowski-distance",
    "href": "dists.html#the-minkowski-distance",
    "title": "1. Distances & Dissimilarities",
    "section": "The Minkowski distance",
    "text": "The Minkowski distance\nDefinition (Minkowski distance on \\(\\mathbb{R}^d\\)): The Minkowski distance between two points \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d\\) is defined as follows: \\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\| \\mathbf{x} - \\mathbf{y} \\|_M = \\left(\\sum\\limits_{j=1}^d \\lvert x_j - y_j\\rvert^p\\right)^{\\frac{1}{p}}, \\quad p \\geq 1.\n\\] You can see that the Minkowski distance is the Manhattan distance for \\(p=1\\) and the Euclidean distance for \\(p=2\\). The Minkowski distance is a generalisation of both!\n\nüí° Fun fact\nWhen \\(p \\rightarrow \\infty\\), the Minkowski distance becomes equivalent to: \\[\n\\lim\\limits_{p \\rightarrow \\infty} \\left( \\sum\\limits_{j=1}^d \\lvert x_j - y_j \\rvert^p \\right)^{\\frac{1}{p}} = \\max\\limits_{1 \\leq j \\leq d} \\lvert x_j - y_j \\rvert,\n\\] which is known as the Chebyshev distance; can you prove this result?\n\nThe Figure below displays the ‚Äúunit circles‚Äù for the Minkowski distance with varying \\(p\\). Notice that \\(p = 0.5\\) is also included; this does not define a proper distance function.\n\n\n\nUnit circles for the Minkowski distance with varying p values. Source: Author‚Äôs own work."
  },
  {
    "objectID": "dists.html#dissimilarities-vs.-distances",
    "href": "dists.html#dissimilarities-vs.-distances",
    "title": "1. Distances & Dissimilarities",
    "section": "Dissimilarities vs.¬†Distances",
    "text": "Dissimilarities vs.¬†Distances\nDefinition (Dissimilarity Function): A dissimilarity is a function \\(d: M \\times M \\rightarrow \\mathbb{R}\\), where \\(M\\) is a set, satisfying the following:\n\n\\(d(x, y) \\geq 0 \\forall x, y \\in M\\) (Non-negativity),\n\\(d(x,y) = d(y, x) \\forall x,y \\in M\\) (Symmetry),\n\\(d(x, y) = 0 \\iff x = y\\) (Indiscernibles)\n\nA dissimilarity function relaxes the requirement for the triangle inequality to hold. This is precisely what distinguishes a dissimilarity from a distance function. Therefore, all distance functions are valid dissimilarity functions. However, the opposite direction is not always true!\nExample: The Minkowski function with \\(p=0.5\\) is a dissimilarity function but not a distance function.\nLet‚Äôs prove this together; firstly, we note that the 3 shared conditions (non-negativity, symmetry, indiscernibles) are trivially true. We only need to check for the triangle inequality.\nLet us take \\(\\mathbf{x} = (0, 0)^\\top\\), \\(\\mathbf{y} = (1, 1)^\\top\\) and \\(\\mathbf{z} = (0, 1)^\\top\\). Then: \\[\n\\begin{align}\nd_M(\\mathbf{x}, \\mathbf{y}) = \\left(\\sum\\limits_{j=1}^d \\lvert x_j - y_j\\rvert^\\frac{1}{2}\\right)^{2} = (1 + 1)^2 = 4,\\\\\nd_M(\\mathbf{x}, \\mathbf{z}) = \\left(\\sum\\limits_{j=1}^d \\lvert x_j - y_j\\rvert^\\frac{1}{2}\\right)^{2} = (0 + 1)^2 = 1,\\\\\nd_M(\\mathbf{y}, \\mathbf{z}) = \\left(\\sum\\limits_{j=1}^d \\lvert x_j - y_j\\rvert^\\frac{1}{2}\\right)^{2} = (0 + 1)^2 = 1.\n\\end{align}\n\\] Therefore, \\(d_M(\\mathbf{x}, \\mathbf{y}) &gt; d_M(\\mathbf{x}, \\mathbf{z}) + d_M(\\mathbf{y}, \\mathbf{z})\\), which means that the triangle inequality does not hold. Therefore, the Minkowski distance with \\(p = 0.5\\) is a dissimilarity but not a distance function. \\(\\square\\)\n\nüí° Fun fact\nThe Minkowski function with \\(0 &lt; p &lt; 1\\) is a dissimilarity function but not a proper metric (can you prove this?), whereas for \\(p \\geq 1\\), it is a distance function.\n\n\n\n\n\n\n\nNoteü§î Quiz: Some of the following functions are valid distance functions, while others are dissimilarities but not distances. Which of these are distance functions?\n\n\n\nA. \\(d(\\mathbf{x}, \\mathbf{y}) = \\sum\\limits_{j=1}^d \\frac{\\lvert x_j - y_j \\rvert}{\\lvert x_j \\rvert + \\lvert y_j \\rvert}\\).\nB. \\(d(\\mathbf{x}, \\mathbf{y}) = 1 - \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\| \\mathbf{x} \\| \\| \\mathbf{y} \\|}\\).\nC. \\(d(\\mathbf{x}, \\mathbf{y}) = \\sum\\limits_{j=1}^d \\mathbb{I}\\{ x_j \\neq y_j\\}\\) for binary vectors \\(\\mathbf{x}, \\mathbf{y}\\).\nD. \\(d(\\mathbf{x}, \\mathbf{y}) = \\min\\limits_{1 \\leq j \\leq d} \\lvert x_j - y_j \\rvert\\).\nE. \\(d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\left(\\mathbf{x} - \\mathbf{y}\\right)^\\top \\mathbf{S}^{-1}\\left(\\mathbf{x} - \\mathbf{y}\\right)}\\), where \\(\\mathbf{S} \\in \\mathbb{R}^{d \\times d}\\) is positive definite.\n\n\n\nShow Answer\n\n\nThis answer is currently in an entangled quantum state with your understanding.\nObserve it too soon and you‚Äôll collapse the learning wave function. ‚öõÔ∏è"
  },
  {
    "objectID": "dists.html#similarities",
    "href": "dists.html#similarities",
    "title": "1. Distances & Dissimilarities",
    "section": "Similarities",
    "text": "Similarities\nDefinition (Similarity Function): Given a dissimilarity function \\(d: M \\times M \\rightarrow \\mathbb{R}\\), a similarity function is a defined as \\(s : M \\times M \\rightarrow [0, 1]\\) with \\(s(\\mathbf{x}, \\mathbf{y}) = f(d(\\mathbf{x}, \\mathbf{y}))\\), where \\(f:[0, \\infty) \\rightarrow [0, 1]\\) is a decreasing function with \\(f(0) = 1\\) and \\(\\lim\\limits_{t \\rightarrow \\infty} f(t) = 0\\).\nExample: Let \\(f(t) = \\exp\\left( - \\frac{t^2}{2\\gamma^2} \\right)\\) for \\(\\gamma &gt; 0\\). The following function is a valid similarity: \\[\ns(\\mathbf{x}, \\mathbf{y}) = f(d_E(\\mathbf{x}, \\mathbf{y})) = \\exp\\left\\{ - \\frac{\\sum\\limits_{j=1}^d (x_j - y_j)^2}{2\\gamma^2} \\right\\}, \\gamma &gt; 0, \\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d.\n\\] This is also known as the Gaussian kernel or Radial Basis Function (RBF) kernel (you do not need to know what a kernel is at this point, but it is a very useful concept in machine learning).\nOther options for \\(f\\) include:\n\n\\(f(t) = \\exp(-t)\\)\n\\(f(t) = \\frac{1}{1 + t}\\)\n\\(f(t) = \\frac{1}{1 + t^2}\\)\n\\(f(t) = \\max \\left(0, 1 - \\frac{t}{\\tau}\\right), \\ \\tau &gt; 0\\)\n\\(f(t) = 1- \\frac{2}{\\pi}\\arctan (\\alpha t), \\ \\alpha &gt; 0\\)\n\n\n\n\n\n\n\nNoteü§î Quiz: Is the following function a valid option for defining a proper similarity function?\n\n\n\n\\[\nf(t) =\n\\begin{cases}\n1, & \\text{if } t = 0, \\\\\n0, & \\text{otherwise.}  \n\\end{cases}\n\\]\nA. Yes\nB. No\n\n\n\nShow Answer\n\n\nIf I give you the answer now, what‚Äôs the similarity between actual learning and just reading answers?\nExactly: \\(f(\\text{similarity}) = 0\\). Check back after class! üòâ"
  },
  {
    "objectID": "dists.html#computing-distances-in-r",
    "href": "dists.html#computing-distances-in-r",
    "title": "1. Distances & Dissimilarities",
    "section": "Computing distances in R",
    "text": "Computing distances in R\nThe dist function provides an easy and quick way of computing distances (or dissimilarities) in R. The output is a distance/dissimilarity matrix, which you can also visualise! Below is a toy example with 50 randomly generated 2-dimensional vectors, where each component is drawn from a standard Uniform distribution.\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Generate 50 2D random vectors = 100 standard uniform values\n# Store in a 50 x 2 matrix\nX &lt;- matrix(data = runif(n = 100, min = 0, max = 1),\n            nrow = 50,\n            ncol = 2)\n\ndist_mat &lt;- dist(x = X, method = \"euclidean\")\ndist_matrix &lt;- as.matrix(dist_mat, nrow = nrow(X), ncol = nrow(X))\n\n# Create layout with space for legend\nlayout(matrix(c(1, 2), nrow = 1), widths = c(4, 1))\n\n# Adjust margins for main plot (bottom, left, top, right)\npar(mar = c(5, 5, 4, 1))\n\n# Main plot with custom palette\nn_colors &lt;- 256\ncolors &lt;- hcl.colors(n_colors, palette = \"YlOrRd\")\n\nimage(x = 1:nrow(dist_matrix),\n      y = 1:ncol(dist_matrix),\n      z = dist_matrix,\n      col = colors,\n      xlab = \"Observation\",\n      ylab = \"Observation\",\n      main = \"Euclidean Distance Matrix\",\n      axes = FALSE)\naxis(1, at = seq(1, 50, by = 5))\naxis(2, at = seq(1, 50, by = 5), las = 1)\nbox()\n\n# Color bar legend with matching colors\npar(mar = c(5, 1, 4, 4))\nlegend_values &lt;- seq(min(dist_matrix), max(dist_matrix), length.out = n_colors)\nimage(x = 1, \n      y = legend_values, \n      z = matrix(legend_values, nrow = 1),\n      col = colors, \n      axes = FALSE, \n      xlab = \"\", \n      ylab = \"\")\naxis(4, las = 1)\nmtext(\"Distance\", side = 4, line = 2.5)\n\n\n\n\n\n\n\n\nThe image function produces a heat map but notice how it requires an argument of class matrix. Therefore, we make sure to convert our distance matrix object to a matrix object and specify the number of rows and columns, which has to be equal to \\(n\\).\n\nüí° Fun fact\nThe dist function supports several distance metrics. Explore its documentation by typing ?dist on the console and see what other distance functions can be implemented, besides the Euclidean. How would you compute the dissimilarity matrix for a dissimilarity function that is not included in the documentation?\n\n\nüöÄ Time to practice!\nGenerate some synthetic data (e.g.¬†by sampling from a uniform or a Normal distribution) and use the dist function with any distance function of your choice. Repeat this process but keep increasing the number of dimensions. What do you observe?\n(Hint: Use the image function to visualise the distance matrix each time)."
  },
  {
    "objectID": "dists.html#key-takeaways",
    "href": "dists.html#key-takeaways",
    "title": "1. Distances & Dissimilarities",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nClustering algorithms use distances, dissimilarities, or similarities to assign objects into groups.\nPoints with low pairwise distances/dissimilarities are commonly assigned in the same cluster, whereas highly distant/dissimilar points are in distinct clusters.\nAll distances are dissimilarities but not the other way round.\nThe triangle inequality is what makes a dissimilarity function a proper distance function.\nLarge distance/dissimilarity \\(\\implies\\) low similarity, while low distance/dissimilarity \\(\\implies\\) high similarity.\nThe dist function in R can be used for computing a distance matrix."
  }
]